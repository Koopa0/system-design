# æ¨¡å‹è¨“ç·´å¹³å°è¨­è¨ˆï¼šå¾å¯¦é©—åˆ°ç”Ÿç”¢çš„å®Œæ•´ MLOps

> æœ¬æ–‡æª”æ¡ç”¨è˜‡æ ¼æ‹‰åº•å¼å°è©±æ³•ï¼ˆSocratic Methodï¼‰å‘ˆç¾ç³»çµ±è¨­è¨ˆçš„æ€è€ƒéç¨‹

## Act 1: æ©Ÿå™¨å­¸ç¿’çš„æ··äº‚ç¾ç‹€

**å ´æ™¯**ï¼šEmma çš„è³‡æ–™ç§‘å­¸åœ˜éšŠæ­£åœ¨è¨“ç·´æ¨è–¦æ¨¡å‹ï¼Œä½†é‡åˆ°äº†å¾ˆå¤šå•é¡Œ

**Emma**ï¼šã€Œæˆ‘å€‘çš„ ML å·¥ä½œæµç¨‹ä¸€åœ˜ç³Ÿï¼æ¯å€‹è³‡æ–™ç§‘å­¸å®¶éƒ½åœ¨è‡ªå·±çš„ Jupyter Notebook ä¸Šè¨“ç·´æ¨¡å‹ï¼Œçµæœç„¡æ³•é‡ç¾ï¼Œä¹Ÿä¸çŸ¥é“å“ªå€‹æ¨¡å‹æœ€å¥½...ã€

**David**ï¼šã€Œé€™æ˜¯å¾ˆå…¸å‹çš„ ML åœ˜éšŠå•é¡Œã€‚è®“æˆ‘åˆ—å‡ºä½ å€‘å¯èƒ½é‡åˆ°çš„ç—›é»ï¼šã€

### å¸¸è¦‹å•é¡Œ

**å•é¡Œ 1ï¼šå¯¦é©—è¿½è¹¤æ··äº‚**
```python
# è³‡æ–™ç§‘å­¸å®¶ A çš„ç­†è¨˜æœ¬
model = train_model(lr=0.001, epochs=10)
# æº–ç¢ºåº¦ï¼š87.3%  â† è¨˜åœ¨å“ªè£¡ï¼Ÿç”¨ä»€éº¼è³‡æ–™ï¼Ÿä»€éº¼æ™‚å€™è¨“ç·´çš„ï¼Ÿ

# è³‡æ–™ç§‘å­¸å®¶ B çš„ç­†è¨˜æœ¬ï¼ˆå…©é€±å¾Œï¼‰
model = train_model(lr=0.01, epochs=20)
# æº–ç¢ºåº¦ï¼š88.1%  â† æ¯”ä¸Šæ¬¡å¥½ï¼Ÿé‚„æ˜¯è³‡æ–™è®Šäº†ï¼Ÿ
```

**Sarah**ï¼šã€Œæ²’æœ‰ç³»çµ±åŒ–çš„å¯¦é©—è¿½è¹¤ï¼Œä½ æ°¸é ä¸çŸ¥é“ï¼šã€
- é€™å€‹æ¨¡å‹ç”¨äº†ä»€éº¼è¶…åƒæ•¸ï¼Ÿ
- è¨“ç·´è³‡æ–™æ˜¯å“ªå€‹ç‰ˆæœ¬ï¼Ÿ
- èƒ½é‡ç¾é€™æ¬¡è¨“ç·´å—ï¼Ÿ
- ç‚ºä»€éº¼é€™æ¬¡æ¯”ä¸Šæ¬¡å¥½ï¼Ÿ

**å•é¡Œ 2ï¼šè³‡æ–™ç‰ˆæœ¬æ§åˆ¶ç¼ºå¤±**
```bash
# æ··äº‚çš„è³‡æ–™ç®¡ç†
data/
  train.csv              â† æ˜¯æœ€æ–°çš„å—ï¼Ÿ
  train_v2.csv           â† v2 æœ‰ä»€éº¼æ”¹è®Šï¼Ÿ
  train_final.csv        â† çœŸçš„ final å—ï¼Ÿ
  train_final_v2.csv     â† ...
  train_really_final.csv â† ğŸ˜±
```

**Michael**ï¼šã€ŒGit å¯ä»¥ç®¡ç†ç¨‹å¼ç¢¼ï¼Œä½†ç„¡æ³•æœ‰æ•ˆç®¡ç†å¤§å‹è³‡æ–™é›†ï¼ˆå¹¾ GB åˆ°å¹¾ TBï¼‰ã€‚ã€

**å•é¡Œ 3ï¼šæ¨¡å‹éƒ¨ç½²å›°é›£**
```
è³‡æ–™ç§‘å­¸å®¶ï¼šã€Œåœ¨æˆ‘çš„ç­†è¨˜æœ¬ä¸Šè·‘å¾—å¾ˆå¥½å•Šï¼ã€
å·¥ç¨‹å¸«ï¼šã€Œä½†æˆ‘ä¸çŸ¥é“æ€éº¼æŠŠä½ çš„ Jupyter Notebook éƒ¨ç½²åˆ°ç”Ÿç”¢ç’°å¢ƒ...ã€
è³‡æ–™ç§‘å­¸å®¶ï¼šã€Œä½ éœ€è¦å®‰è£é€™ 20 å€‹å¥—ä»¶ï¼Œç‰ˆæœ¬è¦å®Œå…¨ä¸€æ¨£...ã€
å·¥ç¨‹å¸«ï¼šã€ŒğŸ˜­ã€
```

**å•é¡Œ 4ï¼šè¨“ç·´æ™‚é–“éé•·**
```python
# å–®æ©Ÿè¨“ç·´
model.fit(X_train, y_train, epochs=100)
# é ä¼°æ™‚é–“ï¼š48 å°æ™‚

# å…©å¤©å¾Œ...
# ç™¼ç¾è¶…åƒæ•¸è¨­éŒ¯äº†ï¼Œè¦é‡æ–°è¨“ç·´ ğŸ˜­
```

**Emma**ï¼šã€Œå®Œå…¨èªªä¸­äº†æˆ‘å€‘çš„å•é¡Œï¼æœ‰è§£æ±ºæ–¹æ¡ˆå—ï¼Ÿã€

**David**ï¼šã€Œæœ‰ï¼é€™å°±æ˜¯æˆ‘å€‘è¦å»ºç«‹çš„**æ¨¡å‹è¨“ç·´å¹³å°**ï¼Œæ¶µè“‹å®Œæ•´çš„ MLOps æµç¨‹ã€‚ã€

## Act 2: MLOps æµç¨‹ç¸½è¦½

**Sarah**ï¼šã€ŒMLOps å°±æ˜¯ ML + DevOpsï¼Œç›®æ¨™æ˜¯å°‡æ©Ÿå™¨å­¸ç¿’å·¥ä½œæµç¨‹å·¥ç¨‹åŒ–ã€è‡ªå‹•åŒ–ã€‚ã€

### å®Œæ•´ MLOps æµç¨‹

```
1. è³‡æ–™ç®¡ç†
   åŸå§‹è³‡æ–™ â†’ æ¸…æ´— â†’ ç‰¹å¾µå·¥ç¨‹ â†’ ç‰ˆæœ¬æ§åˆ¶ (DVC)
   â†“

2. å¯¦é©—è¿½è¹¤
   è¶…åƒæ•¸ â†’ è¨“ç·´ â†’ æŒ‡æ¨™è¨˜éŒ„ â†’ æ¨¡å‹å„²å­˜ (MLflow)
   â†“

3. æ¨¡å‹è¨“ç·´
   è³‡æ–™è¼‰å…¥ â†’ åˆ†æ•£å¼è¨“ç·´ â†’ Checkpoint â†’ è©•ä¼°
   â†“

4. æ¨¡å‹é©—è­‰
   é›¢ç·šè©•ä¼° â†’ A/B Testing â†’ æ€§èƒ½ç›£æ§
   â†“

5. æ¨¡å‹éƒ¨ç½²
   æ‰“åŒ… â†’ éƒ¨ç½² (Kubernetes) â†’ ç‰ˆæœ¬ç®¡ç† â†’ å›æ»¾
   â†“

6. æŒçºŒç›£æ§
   é æ¸¬å“è³ª â†’ è³‡æ–™æ¼‚ç§» â†’ æ¨¡å‹é™ç´š â†’ é‡æ–°è¨“ç·´
```

**Michael**ï¼šã€Œé€™å€‹å¹³å°éœ€è¦è§£æ±ºå“ªäº›æ ¸å¿ƒå•é¡Œï¼Ÿã€

**David**ï¼šã€Œå…­å¤§æ ¸å¿ƒèƒ½åŠ›ï¼šã€

### 1. è³‡æ–™ç‰ˆæœ¬æ§åˆ¶ (DVC - Data Version Control)

```bash
# é¡ä¼¼ Gitï¼Œä½†é‡å°å¤§å‹è³‡æ–™é›†
dvc add data/train.csv
dvc push

# å…¶ä»–äººå¯ä»¥æ‹‰å–ç›¸åŒç‰ˆæœ¬çš„è³‡æ–™
dvc pull
```

**ç‚ºä»€éº¼éœ€è¦ï¼Ÿ**
- ç¢ºä¿è¨“ç·´å¯é‡ç¾ï¼šç›¸åŒç¨‹å¼ç¢¼ + ç›¸åŒè³‡æ–™ = ç›¸åŒçµæœ
- è¿½è¹¤è³‡æ–™è®Šæ›´ï¼šçŸ¥é“æ¯å€‹ç‰ˆæœ¬çš„è³‡æ–™æœ‰ä»€éº¼ä¸åŒ
- è³‡æ–™è¡€ç·£è¿½è¹¤ï¼šé€™å€‹ç‰¹å¾µæ˜¯å¾å“ªè£¡ä¾†çš„ï¼Ÿ

### 2. å¯¦é©—è¿½è¹¤ (MLflow Tracking)

```python
import mlflow

with mlflow.start_run():
    # è¨˜éŒ„åƒæ•¸
    mlflow.log_param("learning_rate", 0.001)
    mlflow.log_param("epochs", 100)

    # è¨“ç·´æ¨¡å‹
    model = train_model(lr=0.001, epochs=100)

    # è¨˜éŒ„æŒ‡æ¨™
    mlflow.log_metric("accuracy", 0.873)
    mlflow.log_metric("f1_score", 0.856)

    # å„²å­˜æ¨¡å‹
    mlflow.sklearn.log_model(model, "model")
```

**Web UI å¯ä»¥çœ‹åˆ°ï¼š**
- æ‰€æœ‰å¯¦é©—çš„åƒæ•¸ã€æŒ‡æ¨™
- æ¯”è¼ƒä¸åŒå¯¦é©—
- è¦–è¦ºåŒ–è¨“ç·´æ›²ç·š
- ä¸‹è¼‰ä»»ä½•æ­·å²æ¨¡å‹

### 3. åˆ†æ•£å¼è¨“ç·´

```python
# å–®æ©Ÿè¨“ç·´ï¼š48 å°æ™‚
model.fit(X_train, y_train)

# åˆ†æ•£å¼è¨“ç·´ï¼ˆ4 GPUï¼‰ï¼š12 å°æ™‚
trainer = pl.Trainer(
    devices=4,
    strategy="ddp",  # Distributed Data Parallel
    accelerator="gpu"
)
trainer.fit(model)
```

**Emma**ï¼šã€Œåˆ†æ•£å¼è¨“ç·´æ€éº¼é‹ä½œï¼Ÿã€

**Michael**ï¼šã€Œä¸»è¦æœ‰å…©ç¨®ç­–ç•¥ï¼šã€

#### è³‡æ–™ä¸¦è¡Œ (Data Parallelism)
```
åŸå§‹è³‡æ–™ (1000 ç­†)
    â†“
åˆ†æˆ 4 ä»½ï¼ˆæ¯ä»½ 250 ç­†ï¼‰
    â†“
GPU 1: è™•ç† 1-250    â”
GPU 2: è™•ç† 251-500  â”œâ”€ åŒæ™‚è¨ˆç®—æ¢¯åº¦
GPU 3: è™•ç† 501-750  â”‚
GPU 4: è™•ç† 751-1000 â”˜
    â†“
åˆä½µæ¢¯åº¦ â†’ æ›´æ–°æ¨¡å‹åƒæ•¸
```

#### æ¨¡å‹ä¸¦è¡Œ (Model Parallelism)
```
å¤§å‹æ¨¡å‹ï¼ˆæ”¾ä¸é€²å–®ä¸€ GPUï¼‰
    â†“
Layer 1-10  â†’ GPU 1
Layer 11-20 â†’ GPU 2
Layer 21-30 â†’ GPU 3
Layer 31-40 â†’ GPU 4
    â†“
è³‡æ–™ä¾åºé€šéå„ GPU
```

### 4. è¶…åƒæ•¸å„ªåŒ– (Hyperparameter Tuning)

**Sarah**ï¼šã€Œèˆ‡å…¶æ‰‹å‹•å˜—è©¦è¶…åƒæ•¸ï¼Œä¸å¦‚è‡ªå‹•åŒ–ï¼šã€

```python
import optuna

def objective(trial):
    # å®šç¾©æœå°‹ç©ºé–“
    lr = trial.suggest_float("lr", 1e-5, 1e-1, log=True)
    batch_size = trial.suggest_categorical("batch_size", [16, 32, 64, 128])
    dropout = trial.suggest_float("dropout", 0.1, 0.5)

    # è¨“ç·´æ¨¡å‹
    model = create_model(lr, batch_size, dropout)
    accuracy = train_and_evaluate(model)

    return accuracy

# è‡ªå‹•æœå°‹æœ€ä½³åƒæ•¸
study = optuna.create_study(direction="maximize")
study.optimize(objective, n_trials=100)

print(f"æœ€ä½³åƒæ•¸: {study.best_params}")
print(f"æœ€ä½³æº–ç¢ºåº¦: {study.best_value}")
```

**æœå°‹ç­–ç•¥ï¼š**
- **Grid Search**ï¼šçª®èˆ‰æ‰€æœ‰çµ„åˆï¼ˆæ…¢ä½†å®Œæ•´ï¼‰
- **Random Search**ï¼šéš¨æ©Ÿæ¡æ¨£ï¼ˆå¿«ä½†å¯èƒ½éŒ¯éæœ€å„ªè§£ï¼‰
- **Bayesian Optimization**ï¼šè²è‘‰æ–¯å„ªåŒ–ï¼ˆè°æ˜åœ°æœå°‹ï¼‰
- **TPE (Tree Parzen Estimator)**ï¼šOptuna é è¨­ï¼ˆæ•ˆæœå¥½ï¼‰

### 5. æ¨¡å‹éƒ¨ç½²

```python
# æ‰“åŒ…æ¨¡å‹ç‚º Docker å®¹å™¨
FROM python:3.9
COPY model.pkl /app/
COPY requirements.txt /app/
RUN pip install -r requirements.txt
EXPOSE 8080
CMD ["python", "serve.py"]

# éƒ¨ç½²åˆ° Kubernetes
kubectl apply -f model-deployment.yaml

# æµé‡é€æ­¥åˆ‡æ›ï¼ˆé‡‘çµ²é›€éƒ¨ç½²ï¼‰
v1.0: 90% æµé‡
v1.1: 10% æµé‡  â† æ–°æ¨¡å‹
    â†“ ç›£æ§æŒ‡æ¨™
å¦‚æœ v1.1 è¡¨ç¾è‰¯å¥½ â†’ 100% åˆ‡æ›
å¦‚æœ v1.1 æœ‰å•é¡Œ â†’ å›æ»¾
```

### 6. æ¨¡å‹ç›£æ§

```python
# ç›£æ§é æ¸¬å“è³ª
predict_latency = time.time() - start
if predict_latency > 100ms:
    alert("Prediction too slow")

# ç›£æ§è³‡æ–™æ¼‚ç§»
current_distribution = get_feature_distribution(new_data)
training_distribution = load_reference_distribution()

drift_score = calculate_drift(current_distribution, training_distribution)
if drift_score > threshold:
    alert("Data drift detected - consider retraining")
```

**Emma**ï¼šã€Œæ˜ç™½äº†ï¼é€™å°±åƒæ˜¯ç‚ºæ©Ÿå™¨å­¸ç¿’å»ºç«‹ä¸€æ¢ç”Ÿç”¢ç·šã€‚ã€

**David**ï¼šã€Œæ²’éŒ¯ï¼è®“æˆ‘å€‘æ·±å…¥æ¯å€‹ç’°ç¯€çš„æŠ€è¡“ç´°ç¯€ã€‚ã€

## Act 3: å¯¦é©—è¿½è¹¤èˆ‡ç®¡ç†

**Michael**ï¼šã€Œå¯¦é©—è¿½è¹¤æ˜¯ MLOps çš„æ ¸å¿ƒã€‚è®“æˆ‘å€‘çœ‹çœ‹å¦‚ä½•ç”¨ MLflow ç³»çµ±åŒ–ç®¡ç†å¯¦é©—ã€‚ã€

### MLflow å››å¤§å…ƒä»¶

#### 1. MLflow Tracking - è¨˜éŒ„å¯¦é©—

```python
import mlflow
import mlflow.sklearn
from sklearn.ensemble import RandomForestClassifier

# è¨­å®š tracking server
mlflow.set_tracking_uri("http://mlflow-server:5000")

# è¨­å®šå¯¦é©—åç¨±
mlflow.set_experiment("recommendation-model-v2")

with mlflow.start_run(run_name="rf-baseline"):
    # 1. è¨˜éŒ„åƒæ•¸
    params = {
        "n_estimators": 100,
        "max_depth": 10,
        "min_samples_split": 5
    }
    mlflow.log_params(params)

    # 2. è¨“ç·´æ¨¡å‹
    model = RandomForestClassifier(**params)
    model.fit(X_train, y_train)

    # 3. è©•ä¼°
    train_acc = model.score(X_train, y_train)
    val_acc = model.score(X_val, y_val)

    # 4. è¨˜éŒ„æŒ‡æ¨™
    mlflow.log_metric("train_accuracy", train_acc)
    mlflow.log_metric("val_accuracy", val_acc)

    # 5. è¨˜éŒ„æ¨¡å‹
    mlflow.sklearn.log_model(model, "model")

    # 6. è¨˜éŒ„é¡å¤–è³‡è¨Š
    mlflow.log_artifact("feature_importance.png")
    mlflow.set_tag("model_type", "random_forest")
    mlflow.set_tag("author", "emma")
```

**åœ¨ MLflow UI å¯ä»¥çœ‹åˆ°ï¼š**
```
Experiment: recommendation-model-v2
â”œâ”€ Run 1: rf-baseline
â”‚  â”œâ”€ Parameters: n_estimators=100, max_depth=10
â”‚  â”œâ”€ Metrics: train_accuracy=0.95, val_accuracy=0.87
â”‚  â””â”€ Artifacts: model/, feature_importance.png
â”œâ”€ Run 2: rf-deep
â”‚  â”œâ”€ Parameters: n_estimators=200, max_depth=20
â”‚  â””â”€ Metrics: train_accuracy=0.98, val_accuracy=0.86 (éæ“¬åˆ!)
â””â”€ Run 3: rf-optimized
   â””â”€ Metrics: val_accuracy=0.89 (æœ€ä½³!)
```

#### 2. MLflow Projects - å¯é‡ç¾çš„åŸ·è¡Œç’°å¢ƒ

```yaml
# MLproject æª”æ¡ˆ
name: recommendation-model

conda_env: conda.yaml

entry_points:
  main:
    parameters:
      learning_rate: {type: float, default: 0.001}
      epochs: {type: int, default: 100}
      data_path: {type: string}
    command: "python train.py --lr {learning_rate} --epochs {epochs} --data {data_path}"
```

```yaml
# conda.yaml - ç’°å¢ƒå®šç¾©
name: ml-env
dependencies:
  - python=3.9
  - scikit-learn=1.0.2
  - pandas=1.4.0
  - numpy=1.22.0
  - pip:
    - mlflow==2.0.1
```

**åŸ·è¡Œï¼š**
```bash
# æœ¬åœ°åŸ·è¡Œ
mlflow run . -P learning_rate=0.01

# é ç«¯åŸ·è¡Œï¼ˆåœ¨ Kubernetes ä¸Šï¼‰
mlflow run . --backend kubernetes -P learning_rate=0.01

# é‡ç¾æ­·å²å¯¦é©—
mlflow run git@github.com:org/project.git -v <commit-hash>
```

#### 3. MLflow Models - æ¨¡å‹æ‰“åŒ…

```python
# è¨˜éŒ„æ¨¡å‹æ™‚è‡ªå‹•ç”Ÿæˆæ¨™æº–æ ¼å¼
mlflow.sklearn.log_model(
    model,
    "model",
    signature=mlflow.models.signature.infer_signature(X_train, predictions),
    input_example=X_train[:5]
)

# ç”Ÿæˆçš„ç›®éŒ„çµæ§‹ï¼š
# model/
# â”œâ”€â”€ MLmodel              â† æ¨¡å‹å…ƒè³‡æ–™
# â”œâ”€â”€ model.pkl            â† å¯¦éš›æ¨¡å‹
# â”œâ”€â”€ conda.yaml           â† ç’°å¢ƒä¾è³´
# â”œâ”€â”€ requirements.txt
# â””â”€â”€ python_env.yaml
```

**è¼‰å…¥ä¸¦ä½¿ç”¨æ¨¡å‹ï¼š**
```python
# æ–¹å¼ 1ï¼šPython å‡½å¼
model = mlflow.sklearn.load_model("runs:/<run-id>/model")
predictions = model.predict(X_new)

# æ–¹å¼ 2ï¼šå•Ÿå‹• REST API æœå‹™
mlflow models serve -m "runs:/<run-id>/model" -p 5001

# æ–¹å¼ 3ï¼šéƒ¨ç½²åˆ°ç”Ÿç”¢ç’°å¢ƒ
mlflow deployments create -t sagemaker -m "runs:/<run-id>/model"
```

#### 4. MLflow Model Registry - æ¨¡å‹ç‰ˆæœ¬ç®¡ç†

```python
# è¨»å†Šæ¨¡å‹
mlflow.register_model(
    "runs:/<run-id>/model",
    "recommendation-model"
)

# æ¨¡å‹ç‰ˆæœ¬ç”Ÿå‘½é€±æœŸ
client = mlflow.tracking.MlflowClient()

# Version 1 â†’ Staging
client.transition_model_version_stage(
    name="recommendation-model",
    version=1,
    stage="Staging"
)

# é©—è­‰é€šé â†’ Production
client.transition_model_version_stage(
    name="recommendation-model",
    version=1,
    stage="Production"
)

# æ–°ç‰ˆæœ¬ä¸Šç·šï¼ŒèˆŠç‰ˆæœ¬ â†’ Archived
client.transition_model_version_stage(
    name="recommendation-model",
    version=0,
    stage="Archived"
)
```

**Sarah**ï¼šã€Œé€™æ¨£å°±èƒ½æ¸…æ¥šè¿½è¹¤æ¯å€‹æ¨¡å‹çš„ç‹€æ…‹äº†ï¼ã€

## Act 4: è³‡æ–™ç‰ˆæœ¬æ§åˆ¶ (DVC)

**Emma**ï¼šã€Œå¯¦é©—å¯ä»¥è¿½è¹¤äº†ï¼Œä½†è³‡æ–™æ€éº¼è¾¦ï¼ŸGit ç„¡æ³•è™•ç†å¤§æª”æ¡ˆã€‚ã€

**David**ï¼šã€ŒDVC (Data Version Control) å°±æ˜¯ç‚ºæ­¤è¨­è¨ˆçš„ï¼ã€

### DVC é‹ä½œåŸç†

```bash
# 1. åˆå§‹åŒ– DVC
dvc init

# 2. è¿½è¹¤è³‡æ–™æª”æ¡ˆ
dvc add data/train.csv

# ç”Ÿæˆå…©å€‹æª”æ¡ˆï¼š
# data/train.csv.dvc  â† æŒ‡æ¨™æª”æ¡ˆï¼ˆå°ï¼Œå¯æ”¾ Gitï¼‰
# data/.gitignore     â† å¿½ç•¥åŸå§‹æª”æ¡ˆ
```

**train.csv.dvc å…§å®¹ï¼š**
```yaml
outs:
- md5: 3c2e5a8f9b7d1c4e6f8a9b0c1d2e3f4a
  size: 1073741824  # 1GB
  path: train.csv
```

```bash
# 3. æ¨é€è³‡æ–™åˆ°é ç«¯å„²å­˜ï¼ˆS3/GCS/Azureï¼‰
dvc remote add -d myremote s3://my-bucket/dvc-storage
dvc push

# 4. æäº¤åˆ° Gitï¼ˆåªæäº¤ .dvc æª”æ¡ˆï¼‰
git add data/train.csv.dvc .dvc/config
git commit -m "Add training data v1.0"
git push

# 5. å…¶ä»–äººæ‹‰å–
git pull
dvc pull  # è‡ªå‹•ä¸‹è¼‰å°æ‡‰ç‰ˆæœ¬çš„è³‡æ–™
```

### DVC Pipeline - è³‡æ–™è™•ç†æµç¨‹

```yaml
# dvc.yaml - å®šç¾©è³‡æ–™è™•ç†æµç¨‹
stages:
  prepare:
    cmd: python prepare.py
    deps:
      - data/raw/users.csv
      - data/raw/items.csv
    outs:
      - data/prepared/dataset.csv

  featurize:
    cmd: python featurize.py
    deps:
      - data/prepared/dataset.csv
      - src/features.py
    outs:
      - data/features/train.pkl
      - data/features/test.pkl

  train:
    cmd: python train.py
    deps:
      - data/features/train.pkl
      - src/model.py
    params:
      - train.learning_rate
      - train.epochs
    metrics:
      - metrics.json:
          cache: false
    outs:
      - models/model.pkl
```

```bash
# åŸ·è¡Œæ•´å€‹ pipeline
dvc repro

# DVC æœƒè‡ªå‹•ï¼š
# 1. æª¢æŸ¥å“ªäº›æª”æ¡ˆæ”¹è®Šäº†
# 2. åªé‡æ–°åŸ·è¡Œå—å½±éŸ¿çš„éšæ®µ
# 3. å¿«å–ä¸­é–“çµæœ
```

**Michael**ï¼šã€ŒDVC + Git çš„çµ„åˆï¼šã€
```
Git (ç®¡ç†ç¨‹å¼ç¢¼å’Œå°æª”æ¡ˆ)
â”œâ”€â”€ src/train.py
â”œâ”€â”€ dvc.yaml
â””â”€â”€ data/train.csv.dvc  â† åªæ˜¯æŒ‡æ¨™

DVC (ç®¡ç†å¤§å‹è³‡æ–™)
â””â”€â”€ S3/GCS
    â””â”€â”€ train.csv  â† çœŸå¯¦è³‡æ–™ï¼ˆ1GBï¼‰
```

**å„ªå‹¢ï¼š**
- âœ… å®Œæ•´çš„è³‡æ–™è¡€ç·£è¿½è¹¤
- âœ… å¯é‡ç¾æ€§ï¼šç¨‹å¼ç¢¼ç‰ˆæœ¬ + è³‡æ–™ç‰ˆæœ¬
- âœ… é«˜æ•ˆå„²å­˜ï¼šå»é‡ã€å£“ç¸®
- âœ… åœ˜éšŠå”ä½œï¼šå…±äº«è³‡æ–™é›†

## Act 5: åˆ†æ•£å¼è¨“ç·´ç­–ç•¥

**Sarah**ï¼šã€Œè¨“ç·´å¤§å‹æ¨¡å‹æ™‚ï¼Œå–®æ©Ÿå¯èƒ½è¦è·‘å¥½å¹¾å¤©ã€‚åˆ†æ•£å¼è¨“ç·´æ€éº¼åšï¼Ÿã€

**David**ï¼šã€Œä¸»è¦æœ‰ä¸‰ç¨®ç­–ç•¥ï¼šData Parallelismã€Model Parallelism å’Œ Pipeline Parallelismã€‚ã€

### ç­–ç•¥ 1ï¼šè³‡æ–™ä¸¦è¡Œ (Data Parallelism)

```python
import torch
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP

# åˆå§‹åŒ–åˆ†æ•£å¼ç’°å¢ƒ
dist.init_process_group(backend='nccl')

# å»ºç«‹æ¨¡å‹
model = MyModel().to(device)
model = DDP(model, device_ids=[local_rank])

# è³‡æ–™åˆ†æ•£
sampler = torch.utils.data.distributed.DistributedSampler(dataset)
dataloader = DataLoader(dataset, sampler=sampler, batch_size=32)

# è¨“ç·´è¿´åœˆ
for epoch in range(num_epochs):
    sampler.set_epoch(epoch)  # ç¢ºä¿æ¯å€‹ epoch è³‡æ–™ä¸åŒ

    for batch in dataloader:
        # æ¯å€‹ GPU è™•ç†ä¸åŒçš„ batch
        outputs = model(batch)
        loss = criterion(outputs, labels)
        loss.backward()

        # DDP è‡ªå‹•åŒæ­¥æ¢¯åº¦
        optimizer.step()
        optimizer.zero_grad()
```

**é‹ä½œæµç¨‹ï¼š**
```
å‡è¨­ 4 å€‹ GPUï¼Œbatch_size=32

åŸå§‹ batch (128 ç­†)
    â†“ è‡ªå‹•åˆ†å‰²
GPU 0: batch[0:32]   â”
GPU 1: batch[32:64]  â”œâ”€ åŒæ™‚å‰å‘å‚³æ’­
GPU 2: batch[64:96]  â”‚
GPU 3: batch[96:128] â”˜
    â†“
æ¯å€‹ GPU è¨ˆç®—è‡ªå·±çš„æ¢¯åº¦
    â†“
All-Reduce: æ‰€æœ‰æ¢¯åº¦æ±‚å¹³å‡
    â†“
æ¯å€‹ GPU ç”¨ç›¸åŒçš„æ¢¯åº¦æ›´æ–°æ¨¡å‹
```

**PyTorch Lightning ç°¡åŒ–ç‰ˆæœ¬ï¼š**
```python
import pytorch_lightning as pl

class MyModel(pl.LightningModule):
    def __init__(self):
        super().__init__()
        self.model = nn.Linear(100, 10)

    def training_step(self, batch, batch_idx):
        x, y = batch
        y_hat = self.model(x)
        loss = F.cross_entropy(y_hat, y)
        return loss

    def configure_optimizers(self):
        return torch.optim.Adam(self.parameters())

# è‡ªå‹•è™•ç†åˆ†æ•£å¼è¨“ç·´
trainer = pl.Trainer(
    devices=4,              # 4 å€‹ GPU
    strategy="ddp",         # Distributed Data Parallel
    accelerator="gpu",
    max_epochs=10
)

trainer.fit(model, train_dataloader)
```

### ç­–ç•¥ 2ï¼šæ¨¡å‹ä¸¦è¡Œ (Model Parallelism)

**Michael**ï¼šã€Œç•¶æ¨¡å‹å¤ªå¤§ï¼Œç„¡æ³•æ”¾é€²å–®ä¸€ GPU æ™‚ä½¿ç”¨ã€‚ã€

```python
import torch.nn as nn

class LargeModel(nn.Module):
    def __init__(self):
        super().__init__()
        # ç¬¬ä¸€éƒ¨åˆ†æ”¾ GPU 0
        self.layer1 = nn.Linear(1000, 1000).to('cuda:0')
        self.layer2 = nn.Linear(1000, 1000).to('cuda:0')

        # ç¬¬äºŒéƒ¨åˆ†æ”¾ GPU 1
        self.layer3 = nn.Linear(1000, 1000).to('cuda:1')
        self.layer4 = nn.Linear(1000, 10).to('cuda:1')

    def forward(self, x):
        # è³‡æ–™å…ˆåœ¨ GPU 0 è™•ç†
        x = x.to('cuda:0')
        x = F.relu(self.layer1(x))
        x = F.relu(self.layer2(x))

        # ç§»åˆ° GPU 1 ç¹¼çºŒ
        x = x.to('cuda:1')
        x = F.relu(self.layer3(x))
        x = self.layer4(x)
        return x
```

**å•é¡Œï¼šGPU åˆ©ç”¨ç‡ä½ï¼**
```
æ™‚é–“ â†’
GPU 0: [â– â– â– â– â–     ]  â† layer1, layer2 é‹ç®—å¾Œé–’ç½®
GPU 1: [     â– â– â– â– ]  â† ç­‰å¾… GPU 0 å®Œæˆ
```

### ç­–ç•¥ 3ï¼šPipeline ä¸¦è¡Œ

**Emma**ï¼šã€Œå¦‚ä½•æå‡åˆ©ç”¨ç‡ï¼Ÿã€

**David**ï¼šã€ŒæŠŠ batch åˆ‡æˆ micro-batchesï¼Œæµæ°´ç·šè™•ç†ï¼ã€

```python
from torch.distributed.pipeline.sync import Pipe

# å®šç¾©æ¨¡å‹å„å±¤
layer1 = nn.Linear(1000, 1000).to('cuda:0')
layer2 = nn.Linear(1000, 1000).to('cuda:0')
layer3 = nn.Linear(1000, 1000).to('cuda:1')
layer4 = nn.Linear(1000, 10).to('cuda:1')

model = nn.Sequential(layer1, layer2, layer3, layer4)

# å•Ÿç”¨ Pipeline ä¸¦è¡Œ
model = Pipe(model, chunks=8)  # æŠŠ batch åˆ‡æˆ 8 å€‹ micro-batches

# è¨“ç·´
for batch in dataloader:
    outputs = model(batch)
    loss = criterion(outputs, labels)
    loss.backward()
```

**æµæ°´ç·šè™•ç†ï¼š**
```
æ™‚é–“ â†’
       Micro-batch:  1   2   3   4   5   6   7   8
GPU 0 (layer 1-2): [â–   ][â–   ][â–   ][â–   ][â–   ][â–   ][â–   ][â–   ]
GPU 1 (layer 3-4):    [â–   ][â–   ][â–   ][â–   ][â–   ][â–   ][â–   ][â–   ]

GPU åˆ©ç”¨ç‡å¤§å¹…æå‡ï¼
```

### æ··åˆç­–ç•¥ï¼šæ•¸æ“š + æ¨¡å‹ä¸¦è¡Œ

**å°æ–¼è¶…å¤§æ¨¡å‹ï¼ˆå¦‚ GPT-3ï¼‰ï¼š**
```python
# 8 å€‹ç¯€é»ï¼Œæ¯å€‹ç¯€é» 4 å€‹ GPU = 32 GPU
# æ¨¡å‹åˆ‡æˆ 4 ä»½ï¼ˆæ¨¡å‹ä¸¦è¡Œï¼‰
# æ¯ä»½åœ¨ 8 å€‹ GPU ä¸Šåšè³‡æ–™ä¸¦è¡Œ

trainer = pl.Trainer(
    devices=4,
    num_nodes=8,
    strategy="deepspeed_stage_3",  # æ··åˆç­–ç•¥
    precision=16  # æ··åˆç²¾åº¦è¨“ç·´ï¼Œæ¸›å°‘è¨˜æ†¶é«”
)
```

**Michael**ï¼šã€Œç¸½çµåˆ†æ•£å¼è¨“ç·´ç­–ç•¥ï¼šã€

| ç­–ç•¥ | é©ç”¨å ´æ™¯ | åŠ é€Ÿæ¯” | è¤‡é›œåº¦ |
|------|----------|--------|--------|
| **è³‡æ–™ä¸¦è¡Œ** | æ¨¡å‹å°ï¼Œè³‡æ–™å¤š | æ¥è¿‘ç·šæ€§ (4 GPU â‰ˆ 4x) | ä½ |
| **æ¨¡å‹ä¸¦è¡Œ** | æ¨¡å‹å¤§ï¼Œæ”¾ä¸é€²å–® GPU | 1-2xï¼ˆé€šè¨Šé–‹éŠ·å¤§ï¼‰ | ä¸­ |
| **Pipeline ä¸¦è¡Œ** | æ¨¡å‹å¤§ + éœ€é«˜åˆ©ç”¨ç‡ | 2-3x | é«˜ |
| **æ··åˆä¸¦è¡Œ** | è¶…å¤§æ¨¡å‹ï¼ˆ> 10B åƒæ•¸ï¼‰ | 10x+ | å¾ˆé«˜ |

## Act 6: è¶…åƒæ•¸å„ªåŒ–è‡ªå‹•åŒ–

**Sarah**ï¼šã€Œæ‰‹å‹•èª¿åƒå¤ªæ…¢äº†ï¼å¦‚ä½•è‡ªå‹•åŒ–ï¼Ÿã€

**David**ï¼šã€ŒOptuna æ˜¯ç›®å‰æœ€å¼·å¤§çš„è¶…åƒæ•¸å„ªåŒ–æ¡†æ¶ã€‚ã€

### Optuna åŸºç¤ç”¨æ³•

```python
import optuna
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score

def objective(trial):
    # 1. å®šç¾©æœå°‹ç©ºé–“
    params = {
        'n_estimators': trial.suggest_int('n_estimators', 10, 200),
        'max_depth': trial.suggest_int('max_depth', 2, 32),
        'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),
        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),
    }

    # 2. è¨“ç·´æ¨¡å‹
    model = RandomForestClassifier(**params, random_state=42)

    # 3. äº¤å‰é©—è­‰è©•ä¼°
    score = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy').mean()

    return score

# 4. å»ºç«‹ study ä¸¦å„ªåŒ–
study = optuna.create_study(
    direction='maximize',  # æœ€å¤§åŒ–æº–ç¢ºåº¦
    sampler=optuna.samplers.TPESampler(),  # ä½¿ç”¨ TPE æ¼”ç®—æ³•
    pruner=optuna.pruners.MedianPruner()   # æå‰åœæ­¢è¡¨ç¾å·®çš„è©¦é©—
)

study.optimize(objective, n_trials=100)

# 5. æŸ¥çœ‹çµæœ
print(f"æœ€ä½³åƒæ•¸: {study.best_params}")
print(f"æœ€ä½³åˆ†æ•¸: {study.best_value}")

# 6. è¦–è¦ºåŒ–
optuna.visualization.plot_optimization_history(study)
optuna.visualization.plot_param_importances(study)
```

### é€²éšåŠŸèƒ½ï¼šæå‰åœæ­¢ (Pruning)

```python
import optuna
from pytorch_lightning.callbacks import Callback

class PyTorchLightningPruningCallback(Callback):
    def __init__(self, trial, monitor):
        self.trial = trial
        self.monitor = monitor

    def on_validation_end(self, trainer, pl_module):
        epoch = trainer.current_epoch
        current_score = trainer.callback_metrics.get(self.monitor)

        # å ±å‘Šç•¶å‰åˆ†æ•¸
        self.trial.report(current_score, epoch)

        # åˆ¤æ–·æ˜¯å¦è©²åœæ­¢
        if self.trial.should_prune():
            raise optuna.TrialPruned()

def objective(trial):
    # å»ºè­°è¶…åƒæ•¸
    lr = trial.suggest_float('lr', 1e-5, 1e-1, log=True)
    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64])

    model = MyModel(lr=lr)

    trainer = pl.Trainer(
        max_epochs=50,
        callbacks=[PyTorchLightningPruningCallback(trial, 'val_accuracy')]
    )

    trainer.fit(model)

    return trainer.callback_metrics['val_accuracy'].item()

# è‡ªå‹•åœæ­¢è¡¨ç¾å·®çš„è©¦é©—ï¼Œç¯€çœæ™‚é–“
study.optimize(objective, n_trials=100)
```

**æ•ˆæœï¼š**
```
è©¦é©— 1: epoch 1 acc=0.5 â†’ ç¹¼çºŒ
        epoch 2 acc=0.55 â†’ ç¹¼çºŒ
        epoch 3 acc=0.60 â†’ ç¹¼çºŒ
        ...
        epoch 50 acc=0.85 â†’ å®Œæˆ

è©¦é©— 2: epoch 1 acc=0.3 â†’ ç¹¼çºŒ
        epoch 2 acc=0.32 â†’ é ä½æ–¼ä¸­ä½æ•¸ â†’ æå‰åœæ­¢ï¼

ç¯€çœæ™‚é–“ï¼š50 epoch â†’ 2 epoch
```

### åˆ†æ•£å¼è¶…åƒæ•¸å„ªåŒ–

```python
# åœ¨å¤šå°æ©Ÿå™¨ä¸ŠåŒæ™‚æœå°‹
import optuna

# ä½¿ç”¨å…±äº«è³‡æ–™åº«
study = optuna.create_study(
    study_name='distributed-optimization',
    storage='postgresql://user:pass@host/dbname',
    load_if_exists=True
)

# æ©Ÿå™¨ Aã€Bã€C åŒæ™‚åŸ·è¡Œ
study.optimize(objective, n_trials=100)

# Optuna è‡ªå‹•å”èª¿ï¼Œé¿å…é‡è¤‡è©¦é©—
```

**Michael**ï¼šã€Œè¶…åƒæ•¸å„ªåŒ–å»ºè­°ï¼šã€

```
å°æ¨¡å‹ï¼ˆè¨“ç·´å¿« < 1åˆ†é˜ï¼‰:
â†’ ä½¿ç”¨ Grid Search æˆ– Random Searchï¼Œçª®èˆ‰æœå°‹

ä¸­å‹æ¨¡å‹ï¼ˆè¨“ç·´ 10-60åˆ†é˜ï¼‰:
â†’ ä½¿ç”¨ Optuna TPEï¼Œ100-200 æ¬¡è©¦é©—

å¤§å‹æ¨¡å‹ï¼ˆè¨“ç·´ > 1å°æ™‚ï¼‰:
â†’ ä½¿ç”¨ Optuna + Pruningï¼Œ20-50 æ¬¡è©¦é©—
â†’ åˆ†æ•£å¼å„ªåŒ–ï¼ŒåŠ é€Ÿæœå°‹

è¶…å¤§æ¨¡å‹ï¼ˆè¨“ç·´ > 1å¤©ï¼‰:
â†’ æ‰‹å‹•èª¿æ•´ + å°‘é‡é—œéµåƒæ•¸è‡ªå‹•å„ªåŒ–
â†’ åƒè€ƒè«–æ–‡çš„å»ºè­°å€¼
```

## Act 7: æŒçºŒç›£æ§èˆ‡æ”¹é€²

**Emma**ï¼šã€Œæ¨¡å‹éƒ¨ç½²å¾Œå°±çµæŸäº†å—ï¼Ÿã€

**David**ï¼šã€Œä¸ï¼é€™åªæ˜¯é–‹å§‹ã€‚æ¨¡å‹æœƒéš¨æ™‚é–“é™ç´šï¼Œéœ€è¦æŒçºŒç›£æ§ã€‚ã€

### æ¨¡å‹é™ç´šçš„åŸå› 

**1. è³‡æ–™æ¼‚ç§» (Data Drift)**
```python
# è¨“ç·´æ™‚çš„è³‡æ–™åˆ†ä½ˆ
è¨“ç·´è³‡æ–™ï¼ˆ2023 å¹´ï¼‰ï¼š
å¹´é½¡åˆ†ä½ˆï¼šå¹³å‡ 35 æ­²ï¼Œæ¨™æº–å·® 12
æ”¶å…¥åˆ†ä½ˆï¼šå¹³å‡ $50Kï¼Œæ¨™æº–å·® $20K

# ç”Ÿç”¢ç’°å¢ƒçš„è³‡æ–™ï¼ˆ2024 å¹´ï¼‰
æ–°è³‡æ–™ï¼š
å¹´é½¡åˆ†ä½ˆï¼šå¹³å‡ 42 æ­²ï¼Œæ¨™æº–å·® 15  â† æ¼‚ç§»äº†ï¼
æ”¶å…¥åˆ†ä½ˆï¼šå¹³å‡ $60Kï¼Œæ¨™æº–å·® $25K

â†’ æ¨¡å‹é æ¸¬æº–ç¢ºåº¦ä¸‹é™
```

**2. æ¦‚å¿µæ¼‚ç§» (Concept Drift)**
```
ç–«æƒ…å‰ï¼š
è³¼è²·æ¨¡å¼ = f(åƒ¹æ ¼, å“è³ª, å“ç‰Œ)

ç–«æƒ…å¾Œï¼š
è³¼è²·æ¨¡å¼ = f(åƒ¹æ ¼, å“è³ª, å“ç‰Œ, æ˜¯å¦å®…é…, é˜²ç–«ç”¨å“)

â†’ åŸæœ¬çš„ç‰¹å¾µä¸å¤ äº†ï¼Œéœ€è¦é‡æ–°è¨“ç·´
```

**3. æ¨™ç±¤åˆ†ä½ˆæ”¹è®Š**
```
åŸå§‹è¨“ç·´è³‡æ–™ï¼š
æ­£æ¨£æœ¬ 50%, è² æ¨£æœ¬ 50%

ç”Ÿç”¢ç’°å¢ƒï¼š
æ­£æ¨£æœ¬ 80%, è² æ¨£æœ¬ 20%  â† ä¸å¹³è¡¡

â†’ æ¨¡å‹åå‘é æ¸¬æ­£æ¨£æœ¬
```

### ç›£æ§æŒ‡æ¨™

```python
from evidently import ColumnMapping
from evidently.dashboard import Dashboard
from evidently.tabs import DataDriftTab, CatTargetDriftTab

# 1. è³‡æ–™æ¼‚ç§»æª¢æ¸¬
def detect_data_drift(reference_data, current_data):
    dashboard = Dashboard(tabs=[DataDriftTab()])
    dashboard.calculate(reference_data, current_data)

    report = dashboard.show()

    # æª¢æŸ¥å“ªäº›ç‰¹å¾µæ¼‚ç§»äº†
    for feature, drift in report['data_drift']['data'].items():
        if drift['drift_detected']:
            print(f"è­¦å‘Šï¼š{feature} ç™¼ç”Ÿæ¼‚ç§»ï¼")
            print(f"  P-value: {drift['p_value']}")
            print(f"  Drift score: {drift['drift_score']}")

# 2. æ¨¡å‹æ•ˆèƒ½ç›£æ§
class ModelMonitor:
    def __init__(self, model, threshold=0.05):
        self.model = model
        self.threshold = threshold
        self.baseline_metrics = {}

    def set_baseline(self, X_val, y_val):
        """è¨­å®šåŸºæº–æŒ‡æ¨™"""
        preds = self.model.predict(X_val)
        self.baseline_metrics = {
            'accuracy': accuracy_score(y_val, preds),
            'precision': precision_score(y_val, preds),
            'recall': recall_score(y_val, preds),
            'f1': f1_score(y_val, preds)
        }

    def check_performance(self, X_new, y_new):
        """æª¢æŸ¥ç•¶å‰æ•ˆèƒ½"""
        preds = self.model.predict(X_new)
        current_metrics = {
            'accuracy': accuracy_score(y_new, preds),
            'precision': precision_score(y_new, preds),
            'recall': recall_score(y_new, preds),
            'f1': f1_score(y_new, preds)
        }

        # æ¯”è¼ƒ
        for metric, baseline in self.baseline_metrics.items():
            current = current_metrics[metric]
            degradation = baseline - current

            if degradation > self.threshold:
                alert(f"{metric} ä¸‹é™ {degradation:.2%}ï¼Œè€ƒæ…®é‡æ–°è¨“ç·´ï¼")

        return current_metrics

# 3. é æ¸¬åˆ†ä½ˆç›£æ§
def monitor_prediction_distribution(model, X_stream):
    """ç›£æ§é æ¸¬åˆ†ä½ˆæ˜¯å¦æ”¹è®Š"""
    predictions = model.predict_proba(X_stream)

    # è¨ˆç®—é æ¸¬ä¿¡å¿ƒ
    confidence = predictions.max(axis=1)

    # è­¦å‘Šï¼šä½ä¿¡å¿ƒé æ¸¬éå¤š
    low_confidence_ratio = (confidence < 0.6).mean()
    if low_confidence_ratio > 0.3:
        alert(f"30% çš„é æ¸¬ä¿¡å¿ƒ < 0.6ï¼Œæ¨¡å‹å¯èƒ½éœ€è¦é‡æ–°è¨“ç·´")

    # è­¦å‘Šï¼šé æ¸¬åˆ†ä½ˆåæ–œ
    class_distribution = predictions.mean(axis=0)
    if class_distribution.max() > 0.8:
        alert("é æ¸¬éåº¦åå‘æŸä¸€é¡åˆ¥")
```

### è‡ªå‹•é‡æ–°è¨“ç·´æµç¨‹

```python
class AutoRetrainPipeline:
    def __init__(self, model, train_func, threshold=0.05):
        self.model = model
        self.train_func = train_func
        self.threshold = threshold
        self.monitor = ModelMonitor(model, threshold)

    def run(self):
        """æŒçºŒç›£æ§ä¸¦åœ¨éœ€è¦æ™‚é‡æ–°è¨“ç·´"""
        while True:
            # 1. æ”¶é›†æ–°è³‡æ–™
            X_new, y_new = collect_recent_data(days=7)

            # 2. æª¢æŸ¥æ•ˆèƒ½
            metrics = self.monitor.check_performance(X_new, y_new)

            # 3. æª¢æŸ¥è³‡æ–™æ¼‚ç§»
            drift_detected = detect_data_drift(
                self.reference_data,
                X_new
            )

            # 4. æ±ºå®šæ˜¯å¦é‡æ–°è¨“ç·´
            if should_retrain(metrics, drift_detected):
                print("è§¸ç™¼è‡ªå‹•é‡æ–°è¨“ç·´...")

                # 5. æº–å‚™æ–°çš„è¨“ç·´è³‡æ–™ï¼ˆèˆŠè³‡æ–™ + æ–°è³‡æ–™ï¼‰
                X_train = combine_data(self.X_train_old, X_new)
                y_train = combine_data(self.y_train_old, y_new)

                # 6. é‡æ–°è¨“ç·´
                new_model = self.train_func(X_train, y_train)

                # 7. A/B Testing
                if ab_test_passed(self.model, new_model):
                    print("æ–°æ¨¡å‹è¡¨ç¾æ›´å¥½ï¼Œéƒ¨ç½²ä¸Šç·š")
                    self.model = new_model
                    self.monitor.set_baseline(X_new, y_new)
                else:
                    print("æ–°æ¨¡å‹è¡¨ç¾ä¸ä½³ï¼Œä¿ç•™èˆŠæ¨¡å‹")

            # 8. ç­‰å¾…ä¸‹ä¸€å€‹é€±æœŸ
            time.sleep(86400)  # æ¯å¤©æª¢æŸ¥ä¸€æ¬¡

def should_retrain(metrics, drift_detected):
    """é‡æ–°è¨“ç·´è§¸ç™¼æ¢ä»¶"""
    # æ¢ä»¶ 1ï¼šæº–ç¢ºåº¦ä¸‹é™è¶…é 5%
    if metrics['accuracy'] < baseline_accuracy - 0.05:
        return True

    # æ¢ä»¶ 2ï¼šæª¢æ¸¬åˆ°è³‡æ–™æ¼‚ç§»
    if drift_detected:
        return True

    # æ¢ä»¶ 3ï¼šå®šæœŸé‡æ–°è¨“ç·´ï¼ˆæ¯ 30 å¤©ï¼‰
    if days_since_last_training > 30:
        return True

    return False
```

### A/B Testing æ¡†æ¶

```python
class ABTest:
    def __init__(self, model_a, model_b, traffic_split=0.1):
        self.model_a = model_a  # ç•¶å‰æ¨¡å‹
        self.model_b = model_b  # æ–°æ¨¡å‹
        self.traffic_split = traffic_split  # 10% æµé‡çµ¦æ–°æ¨¡å‹
        self.results = {'a': [], 'b': []}

    def predict(self, user_id, features):
        """æ ¹æ“šç”¨æˆ¶ ID æ±ºå®šä½¿ç”¨å“ªå€‹æ¨¡å‹"""
        # ä¸€è‡´æ€§é›œæ¹Šï¼Œç¢ºä¿åŒä¸€ç”¨æˆ¶ç¸½æ˜¯åˆ†é…åˆ°ç›¸åŒæ¨¡å‹
        if hash(user_id) % 100 < self.traffic_split * 100:
            model = self.model_b
            group = 'b'
        else:
            model = self.model_a
            group = 'a'

        prediction = model.predict(features)

        # è¨˜éŒ„çµæœ
        self.results[group].append({
            'user_id': user_id,
            'prediction': prediction,
            'timestamp': datetime.now()
        })

        return prediction

    def evaluate(self, min_samples=1000):
        """è©•ä¼°å…©å€‹æ¨¡å‹çš„è¡¨ç¾"""
        if len(self.results['b']) < min_samples:
            print(f"è³‡æ–™ä¸è¶³ï¼Œéœ€è¦ {min_samples} ç­†")
            return None

        # è¨ˆç®—æŒ‡æ¨™
        metric_a = calculate_metrics(self.results['a'])
        metric_b = calculate_metrics(self.results['b'])

        # çµ±è¨ˆé¡¯è‘—æ€§æª¢é©—
        p_value = statistical_test(metric_a, metric_b)

        if p_value < 0.05 and metric_b > metric_a:
            print("æ–°æ¨¡å‹é¡¯è‘—æ›´å¥½ï¼å»ºè­°å…¨é¢éƒ¨ç½²")
            return 'b'
        elif p_value < 0.05 and metric_b < metric_a:
            print("æ–°æ¨¡å‹è¡¨ç¾è¼ƒå·®ï¼ä¿ç•™èˆŠæ¨¡å‹")
            return 'a'
        else:
            print("å…©å€‹æ¨¡å‹ç„¡é¡¯è‘—å·®ç•°")
            return None
```

**Sarah**ï¼šã€Œç¸½çµ MLOps å®Œæ•´æµç¨‹ï¼šã€

```
è³‡æ–™ç®¡ç†ï¼ˆDVCï¼‰
    â†“
å¯¦é©—è¿½è¹¤ï¼ˆMLflowï¼‰
    â†“
è¶…åƒæ•¸å„ªåŒ–ï¼ˆOptunaï¼‰
    â†“
åˆ†æ•£å¼è¨“ç·´ï¼ˆPyTorch Lightningï¼‰
    â†“
æ¨¡å‹è¨»å†Šï¼ˆMLflow Model Registryï¼‰
    â†“
A/B Testing
    â†“
éƒ¨ç½²ä¸Šç·šï¼ˆKubernetesï¼‰
    â†“
æŒçºŒç›£æ§ï¼ˆEvidently, Prometheusï¼‰
    â†“
è‡ªå‹•é‡æ–°è¨“ç·´ â”€â”€â”˜ (å¾ªç’°)
```

**Emma**ï¼šã€Œé€™æ¨£å°±èƒ½å»ºç«‹ä¸€å€‹å®Œæ•´çš„ã€å¯æŒçºŒçš„æ©Ÿå™¨å­¸ç¿’ç³»çµ±äº†ï¼ã€

**Michael**ï¼šã€Œæ²’éŒ¯ï¼é€™å°±æ˜¯ç¾ä»£åŒ–çš„ MLOps å¹³å°ã€‚ã€

---

## ç¸½çµ

**David**ï¼šã€Œå»ºç«‹æ¨¡å‹è¨“ç·´å¹³å°çš„æ ¸å¿ƒåŸå‰‡ï¼šã€

| åŸå‰‡ | èªªæ˜ | å·¥å…· |
|------|------|------|
| **å¯é‡ç¾æ€§** | ä»»ä½•å¯¦é©—éƒ½èƒ½å®Œæ•´é‡ç¾ | MLflow + DVC |
| **å¯è¿½è¹¤æ€§** | æ¯å€‹æ¨¡å‹çš„ä¾†æºæ¸…æ¥šå¯æŸ¥ | MLflow Tracking |
| **å¯æ“´å±•æ€§** | å¾å–®æ©Ÿåˆ°åˆ†æ•£å¼ç„¡ç¸«æ“´å±• | PyTorch Lightning |
| **è‡ªå‹•åŒ–** | æ¸›å°‘æ‰‹å‹•æ“ä½œï¼Œæå‡æ•ˆç‡ | Optuna + CI/CD |
| **å¯é æ€§** | æ¨¡å‹å“è³ªç©©å®šï¼Œç•°å¸¸å¯å¿«é€Ÿå›æ»¾ | A/B Testing + Monitoring |

**é€éæœ¬ç« å­¸ç¿’ï¼Œä½ æŒæ¡äº†ï¼š**

1. âœ… **å¯¦é©—ç®¡ç†**ï¼šMLflow è¿½è¹¤ã€æ¯”è¼ƒã€é‡ç¾å¯¦é©—
2. âœ… **è³‡æ–™ç‰ˆæœ¬æ§åˆ¶**ï¼šDVC ç®¡ç†å¤§å‹è³‡æ–™é›†
3. âœ… **åˆ†æ•£å¼è¨“ç·´**ï¼šåŠ é€Ÿæ¨¡å‹è¨“ç·´ 4-10 å€
4. âœ… **è¶…åƒæ•¸å„ªåŒ–**ï¼šè‡ªå‹•æœå°‹æœ€ä½³åƒæ•¸
5. âœ… **æŒçºŒç›£æ§**ï¼šæª¢æ¸¬æ¨¡å‹é™ç´šï¼Œè‡ªå‹•é‡æ–°è¨“ç·´
6. âœ… **A/B Testing**ï¼šå®‰å…¨åœ°ä¸Šç·šæ–°æ¨¡å‹
7. âœ… **MLOps æµç¨‹**ï¼šå¾å¯¦é©—åˆ°ç”Ÿç”¢çš„å®Œæ•´é–‰ç’°

**ä¸‹ä¸€ç« **ï¼šæˆ‘å€‘å°‡å­¸ç¿’**æ¨è–¦å¼•æ“**ï¼Œçµåˆæ©Ÿå™¨å­¸ç¿’èˆ‡ç³»çµ±è¨­è¨ˆï¼Œæ‰“é€ å€‹æ€§åŒ–æ¨è–¦ç³»çµ±ã€‚
