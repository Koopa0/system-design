# æ¨è–¦å¼•æ“ (Recommendation Engine)

## ç³»çµ±æ¦‚è¿°

æ¨è–¦å¼•æ“æ˜¯ä¸€å€‹çµåˆå”åŒéæ¿¾ã€å…§å®¹æ¨è–¦ã€æ·±åº¦å­¸ç¿’çš„å€‹æ€§åŒ–æ¨è–¦å¹³å°ï¼Œé€éåˆ†æç”¨æˆ¶è¡Œç‚ºå’Œå•†å“ç‰¹å¾µï¼Œç‚ºæ¯ä½ç”¨æˆ¶æä¾›ç²¾æº–çš„å€‹æ€§åŒ–æ¨è–¦ï¼Œæå‡è½‰æ›ç‡å’Œç”¨æˆ¶æ»¿æ„åº¦ã€‚

### æ ¸å¿ƒèƒ½åŠ›

1. **å¤šç­–ç•¥å¬å›** - å”åŒéæ¿¾ã€å…§å®¹æ¨è–¦ã€ç†±é–€æ¦œå–®ã€å¯¦æ™‚èˆˆè¶£
2. **æ·±åº¦å­¸ç¿’æ’åº** - Two-Towerã€Wide & Deepã€DCN ç­‰å…ˆé€²æ¨¡å‹
3. **å¯¦æ™‚ç‰¹å¾µå·¥ç¨‹** - æ¯«ç§’ç´šç‰¹å¾µè¨ˆç®—ï¼Œæ•æ‰ç”¨æˆ¶å³æ™‚èˆˆè¶£
4. **æ™ºèƒ½é‡æ’åº** - å¤šæ¨£æ€§ã€æ–°é®®åº¦ã€æ¥­å‹™è¦å‰‡å„ªåŒ–
5. **ç·šä¸Šå­¸ç¿’** - Bandit ç®—æ³•ã€A/B Testingã€æŒçºŒå„ªåŒ–
6. **å¯æ“´å±•æ¶æ§‹** - æ”¯æ´å„„ç´šç”¨æˆ¶ã€åƒè¬ç´šå•†å“

### æ¥­å‹™åƒ¹å€¼

| æŒ‡æ¨™ | å„ªåŒ–å‰ | å„ªåŒ–å¾Œ | æå‡ |
|------|--------|--------|------|
| **é»æ“Šç‡ (CTR)** | 2% | 8% | **4Ã—** |
| **è½‰æ›ç‡ (CVR)** | 1.5% | 6% | **4Ã—** |
| **ç”¨æˆ¶åœç•™æ™‚é–“** | 3 åˆ†é˜ | 12 åˆ†é˜ | **4Ã—** |
| **GMV** | $100M/æœˆ | $250M/æœˆ | **2.5Ã—** |
| **ç”¨æˆ¶ç•™å­˜ç‡** | 25% | 45% | **+20%** |

### æ‡‰ç”¨å ´æ™¯

- **é›»å•†å¹³å°**ï¼šå•†å“æ¨è–¦ã€äº¤å‰éŠ·å”®ã€å€‹æ€§åŒ–é¦–é 
- **å½±éŸ³ä¸²æµ**ï¼šå½±ç‰‡/éŸ³æ¨‚æ¨è–¦ã€æ’­æ”¾æ¸…å–®ç”Ÿæˆ
- **æ–°èè³‡è¨Š**ï¼šæ–‡ç« æ¨è–¦ã€å€‹æ€§åŒ–è³‡è¨Šæµ
- **ç¤¾äº¤åª’é«”**ï¼šå…§å®¹æ¨è–¦ã€å¥½å‹æ¨è–¦ã€å»£å‘ŠæŠ•æ”¾
- **ç·šä¸Šæ•™è‚²**ï¼šèª²ç¨‹æ¨è–¦ã€å­¸ç¿’è·¯å¾‘è¦åŠƒ

## åŠŸèƒ½éœ€æ±‚

### 1. æ ¸å¿ƒåŠŸèƒ½

#### 1.1 å¬å›å±¤
- å”åŒéæ¿¾å¬å›ï¼ˆUser-CFã€Item-CFã€Matrix Factorizationï¼‰
- å…§å®¹å¬å›ï¼ˆTF-IDFã€BERT Embeddingsï¼‰
- ç†±é–€å¬å›ï¼ˆå…¨ç«™ç†±é–€ã€åˆ†é¡ç†±é–€ã€å¯¦æ™‚ç†±é–€ï¼‰
- ç”¨æˆ¶æ­·å²å¬å›ï¼ˆrecently viewedã€è³¼è²·ç›¸é—œï¼‰
- å¯¦æ™‚èˆˆè¶£å¬å›ï¼ˆsession-basedã€åºåˆ—æ¨¡å‹ï¼‰

#### 1.2 æ’åºå±¤
- ç²—æ’æ¨¡å‹ï¼ˆç°¡å–®æ¨¡å‹å¿«é€Ÿæ‰“åˆ†ï¼‰
- ç²¾æ’æ¨¡å‹ï¼ˆWide & Deepã€DCNã€Two-Towerï¼‰
- å¤šç›®æ¨™å„ªåŒ–ï¼ˆCTRã€CVRã€åœç•™æ™‚é–“ã€åˆ©æ½¤ï¼‰
- å¯¦æ™‚ç‰¹å¾µèåˆ

#### 1.3 é‡æ’åºå±¤
- å¤šæ¨£æ€§å„ªåŒ–ï¼ˆMMRã€DPPï¼‰
- æ–°é®®åº¦æå‡ï¼ˆæ™‚é–“è¡°æ¸›ã€æ–°å“åŠ æ¬Šï¼‰
- æ¥­å‹™è¦å‰‡ï¼ˆåº«å­˜ã€åˆ©æ½¤ç‡ã€é‹ç‡Ÿç­–ç•¥ï¼‰
- å€‹æ€§åŒ–èª¿æ•´ï¼ˆVIP ç”¨æˆ¶ã€åœ°åŸŸå·®ç•°ï¼‰

#### 1.4 ç·šä¸Šå­¸ç¿’
- Contextual Banditï¼ˆThompson Samplingã€UCBï¼‰
- A/B Testing æ¡†æ¶
- å¯¦æ™‚æ¨¡å‹æ›´æ–°
- æ•ˆæœç›£æ§èˆ‡åé¥‹

### 2. éåŠŸèƒ½éœ€æ±‚

| éœ€æ±‚ | æŒ‡æ¨™ | èªªæ˜ |
|------|------|------|
| **éŸ¿æ‡‰å»¶é²** | < 100ms | P99 æ¨è–¦è«‹æ±‚å»¶é² |
| **ååé‡** | 100K QPS | é«˜å³°æœŸè«‹æ±‚æ”¯æ´ |
| **å¬å›è¦æ¨¡** | 1000 è¬+ å•†å“ | å•†å“åº«è¦æ¨¡ |
| **ç”¨æˆ¶è¦æ¨¡** | 1 å„„+ ç”¨æˆ¶ | æ—¥æ´»ç”¨æˆ¶æ”¯æ´ |
| **æ¨¡å‹æ›´æ–°** | æ¯å°æ™‚ | ç·šä¸Šæ¨¡å‹æ›´æ–°é »ç‡ |
| **ç‰¹å¾µå»¶é²** | < 10ms | å¯¦æ™‚ç‰¹å¾µè¨ˆç®—å»¶é² |
| **å¯ç”¨æ€§** | 99.99% | æœå‹™å¯ç”¨æ€§ |

## æŠ€è¡“æ¶æ§‹

### ç³»çµ±æ¶æ§‹åœ–

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          Client Layer                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚   Web    â”‚  â”‚  Mobile  â”‚  â”‚   App    â”‚  â”‚  WeChat  â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         API Gateway                              â”‚
â”‚              (é™æµã€èªè­‰ã€A/B åˆ†æµ)                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Recommendation Service                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚              Request Handler                          â”‚       â”‚
â”‚  â”‚  1. è§£æè«‹æ±‚   2. ç²å–ç”¨æˆ¶ç•«åƒ   3. å”èª¿å¬å›æ’åº     â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                      â”‚                      â”‚
        â–¼                      â–¼                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Recall Layer â”‚   â”‚ Ranking Layerâ”‚   â”‚ Rerank Layer â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                      â”‚                      â”‚
        â–¼                      â–¼                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Recall Service                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚Collab.   â”‚  â”‚ Content  â”‚  â”‚ Popular  â”‚  â”‚ RealTime â”‚       â”‚
â”‚  â”‚Filtering â”‚  â”‚  Based   â”‚  â”‚  Items   â”‚  â”‚ Interest â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                      â†“ 500 candidates                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Ranking Service                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚  Coarse Ranking (500 â†’ 100)                          â”‚       â”‚
â”‚  â”‚  - Simple Model (LR/GBDT)                            â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚  Fine Ranking (100 â†’ 20)                             â”‚       â”‚
â”‚  â”‚  - Deep Model (Wide & Deep / DCN / Two-Tower)       â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                      â†“ Top 20                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Re-ranking Service                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚Diversity â”‚  â”‚Freshness â”‚  â”‚ Business â”‚  â”‚Multi-Obj â”‚       â”‚
â”‚  â”‚   MMR    â”‚  â”‚  Boost   â”‚  â”‚  Rules   â”‚  â”‚Optimize  â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                      â†“ Final Top 10                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â–¼                     â–¼                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Feature     â”‚   â”‚    Model     â”‚   â”‚   Online     â”‚
â”‚  Service     â”‚   â”‚   Service    â”‚   â”‚  Learning    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                      â”‚                      â”‚
        â–¼                      â–¼                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Storage Layer                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚  Redis   â”‚  â”‚PostgreSQLâ”‚  â”‚   HBase  â”‚  â”‚  HDFS    â”‚       â”‚
â”‚  â”‚(å¯¦æ™‚ç‰¹å¾µ)â”‚  â”‚(ç”¨æˆ¶ç•«åƒ)â”‚  â”‚(è¡Œç‚ºæ—¥èªŒ)â”‚  â”‚(é›¢ç·šè¨“ç·´)â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### æŠ€è¡“æ£§

| å±¤ç´š | æŠ€è¡“é¸å‹ | åŸå›  |
|------|----------|------|
| **API æœå‹™** | Go + Gin | é«˜æ•ˆèƒ½ã€ä½å»¶é² |
| **å¬å›** | Faiss / Milvus | å‘é‡æª¢ç´¢ã€ANN æœå°‹ |
| **æ’åº** | TensorFlow Serving | æ·±åº¦å­¸ç¿’æ¨¡å‹éƒ¨ç½² |
| **å¯¦æ™‚ç‰¹å¾µ** | Redis | æ¯«ç§’ç´šè®€å¯« |
| **é›¢ç·šç‰¹å¾µ** | Hive + Spark | å¤§è¦æ¨¡è³‡æ–™è™•ç† |
| **ç”¨æˆ¶ç•«åƒ** | PostgreSQL | çµæ§‹åŒ–è³‡æ–™ |
| **è¡Œç‚ºæ—¥èªŒ** | Kafka + HBase | é«˜ååã€æ™‚åºè³‡æ–™ |
| **æ¨¡å‹è¨“ç·´** | PyTorch + Ray | åˆ†æ•£å¼è¨“ç·´ |
| **A/B Testing** | è‡ªç ”æ¡†æ¶ | éˆæ´»é…ç½® |
| **ç›£æ§** | Prometheus + Grafana | æŒ‡æ¨™æ”¶é›†èˆ‡è¦–è¦ºåŒ– |

## è³‡æ–™åº«è¨­è¨ˆ

### 1. ç”¨æˆ¶ç•«åƒè¡¨ (user_profiles)

```sql
CREATE TABLE user_profiles (
    user_id BIGINT PRIMARY KEY,
    age INTEGER,
    gender VARCHAR(10),
    city VARCHAR(50),
    vip_level INTEGER,
    registration_date DATE,

    -- çµ±è¨ˆç‰¹å¾µ
    total_orders INTEGER DEFAULT 0,
    total_gmv DECIMAL(12, 2) DEFAULT 0,
    avg_order_value DECIMAL(10, 2),
    favorite_categories INTEGER[],  -- æœ€å–œæ­¡çš„é¡åˆ¥ ID é™£åˆ—

    -- å‘é‡ç‰¹å¾µï¼ˆJSON å­˜å„²ï¼‰
    user_embedding JSONB,           -- ç”¨æˆ¶ embedding å‘é‡

    -- æ™‚é–“æˆ³
    created_at TIMESTAMP NOT NULL DEFAULT NOW(),
    updated_at TIMESTAMP NOT NULL DEFAULT NOW()
);

CREATE INDEX idx_user_profiles_city ON user_profiles(city);
CREATE INDEX idx_user_profiles_vip_level ON user_profiles(vip_level);
CREATE INDEX idx_user_profiles_favorite_categories ON user_profiles USING GIN(favorite_categories);
```

### 2. å•†å“è¡¨ (items)

```sql
CREATE TABLE items (
    item_id BIGINT PRIMARY KEY,
    title VARCHAR(500) NOT NULL,
    category_id INTEGER NOT NULL,
    brand_id INTEGER,
    price DECIMAL(10, 2) NOT NULL,
    stock INTEGER DEFAULT 0,
    sales_count INTEGER DEFAULT 0,
    rating DECIMAL(3, 2),

    -- ç‰¹å¾µ
    tags VARCHAR(100)[],
    attributes JSONB,               -- å•†å“å±¬æ€§ï¼ˆé¡è‰²ã€å°ºå¯¸ç­‰ï¼‰
    item_embedding JSONB,           -- å•†å“ embedding å‘é‡

    -- çµ±è¨ˆ
    view_count INTEGER DEFAULT 0,
    click_count INTEGER DEFAULT 0,
    cart_count INTEGER DEFAULT 0,
    purchase_count INTEGER DEFAULT 0,

    -- æ¥­å‹™
    profit_margin DECIMAL(5, 4),    -- åˆ©æ½¤ç‡
    is_new BOOLEAN DEFAULT true,
    is_active BOOLEAN DEFAULT true,

    created_at TIMESTAMP NOT NULL DEFAULT NOW(),
    updated_at TIMESTAMP NOT NULL DEFAULT NOW()
);

CREATE INDEX idx_items_category ON items(category_id);
CREATE INDEX idx_items_brand ON items(brand_id);
CREATE INDEX idx_items_price ON items(price);
CREATE INDEX idx_items_is_active ON items(is_active) WHERE is_active = true;
CREATE INDEX idx_items_tags ON items USING GIN(tags);
```

### 3. ç”¨æˆ¶è¡Œç‚ºè¡¨ (user_behaviors) - HBase

```
RowKey: user_id:timestamp
Column Family: action
Columns:
  - item_id
  - action_type (view/click/cart/purchase)
  - duration (åœç•™æ™‚é–“)
  - device_type
  - context (ä¸Šä¸‹æ–‡è³‡è¨Š)
```

### 4. å•†å“ç›¸ä¼¼åº¦è¡¨ (item_similarities)

```sql
CREATE TABLE item_similarities (
    item_id_1 BIGINT NOT NULL,
    item_id_2 BIGINT NOT NULL,
    similarity_score REAL NOT NULL,
    similarity_type VARCHAR(50),    -- 'collaborative', 'content', 'embedding'
    computed_at TIMESTAMP NOT NULL DEFAULT NOW(),

    PRIMARY KEY (item_id_1, item_id_2)
);

CREATE INDEX idx_item_sim_score ON item_similarities(item_id_1, similarity_score DESC);
```

### 5. æ¨è–¦æ—¥èªŒè¡¨ (recommendation_logs) - HBase

```
RowKey: user_id:request_id
Column Family: request
Columns:
  - timestamp
  - recommended_items (æ¨è–¦çš„å•†å“åˆ—è¡¨)
  - recall_sources (å¬å›ä¾†æº)
  - scores (æ’åºåˆ†æ•¸)

Column Family: feedback
Columns:
  - clicked_items
  - purchased_items
  - dwell_times
```

### 6. A/B Testing é…ç½®è¡¨ (ab_experiments)

```sql
CREATE TABLE ab_experiments (
    experiment_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    name VARCHAR(200) NOT NULL,
    description TEXT,
    status VARCHAR(20) NOT NULL,    -- 'draft', 'running', 'completed'

    -- æµé‡åˆ†é…
    traffic_allocation JSONB,       -- {"control": 0.5, "treatment": 0.5}

    -- é…ç½®
    variants JSONB,                 -- å„è®Šé«”çš„é…ç½®

    -- æ™‚é–“
    start_time TIMESTAMP,
    end_time TIMESTAMP,
    created_at TIMESTAMP NOT NULL DEFAULT NOW()
);

CREATE TABLE ab_assignments (
    user_id BIGINT NOT NULL,
    experiment_id UUID NOT NULL REFERENCES ab_experiments(id),
    variant VARCHAR(50) NOT NULL,   -- 'control', 'treatment'
    assigned_at TIMESTAMP NOT NULL DEFAULT NOW(),

    PRIMARY KEY (user_id, experiment_id)
);

CREATE INDEX idx_ab_assignments_experiment ON ab_assignments(experiment_id);
```

## æ ¸å¿ƒåŠŸèƒ½å¯¦ä½œ

### 1. å”åŒéæ¿¾å¬å›

```python
# recall/collaborative_filtering.py
import numpy as np
from scipy.sparse import csr_matrix
from sklearn.metrics.pairwise import cosine_similarity

class ItemBasedCF:
    def __init__(self):
        self.item_similarity_matrix = None

    def train(self, user_item_matrix):
        """
        user_item_matrix: scipy sparse matrix
        rows = users, cols = items
        """
        # è¨ˆç®— Item-Item ç›¸ä¼¼åº¦çŸ©é™£
        self.item_similarity_matrix = cosine_similarity(
            user_item_matrix.T,  # è½‰ç½®ï¼šitem Ã— user
            dense_output=False
        )

    def recall(self, user_id, user_history, top_k=100):
        """
        ç‚ºç”¨æˆ¶å¬å›å•†å“
        user_history: ç”¨æˆ¶æ­·å²äº’å‹•çš„å•†å“ ID åˆ—è¡¨
        """
        # è¨ˆç®—å€™é¸å•†å“åˆ†æ•¸
        scores = {}

        for item_id in user_history:
            # æ‰¾åˆ°èˆ‡è©²å•†å“ç›¸ä¼¼çš„å•†å“
            similar_items = self.item_similarity_matrix[item_id].toarray()[0]

            for candidate_id, similarity in enumerate(similar_items):
                if candidate_id not in user_history and similarity > 0:
                    if candidate_id not in scores:
                        scores[candidate_id] = 0
                    scores[candidate_id] += similarity

        # æ’åºå– Top-K
        sorted_items = sorted(scores.items(), key=lambda x: x[1], reverse=True)
        return [item_id for item_id, _ in sorted_items[:top_k]]

# ä½¿ç”¨ Spark ALS for large scale
from pyspark.ml.recommendation import ALS

class ALSRecall:
    def __init__(self, rank=100, max_iter=10):
        self.model = None
        self.rank = rank
        self.max_iter = max_iter

    def train(self, ratings_df):
        """
        ratings_df: Spark DataFrame with columns [user_id, item_id, rating]
        """
        als = ALS(
            rank=self.rank,
            maxIter=self.max_iter,
            regParam=0.1,
            userCol="user_id",
            itemCol="item_id",
            ratingCol="rating",
            coldStartStrategy="drop",
            implicitPrefs=True  # éš±å¼åé¥‹ï¼ˆé»æ“Šã€è§€çœ‹ï¼‰
        )

        self.model = als.fit(ratings_df)

    def recall(self, user_id, top_k=100):
        """ç‚ºå–®ä¸€ç”¨æˆ¶å¬å›"""
        user_df = spark.createDataFrame([(user_id,)], ["user_id"])
        recommendations = self.model.recommendForUserSubset(user_df, top_k)

        return recommendations.select("recommendations.item_id").collect()[0][0]
```

### 2. æ·±åº¦å­¸ç¿’æ’åºæ¨¡å‹

```python
# ranking/wide_and_deep.py
import torch
import torch.nn as nn

class WideAndDeepModel(nn.Module):
    def __init__(self, wide_dim, deep_dim, embedding_dims):
        super().__init__()

        # Wide component (ç·šæ€§æ¨¡å‹)
        self.wide = nn.Linear(wide_dim, 1)

        # Embedding layers
        self.embeddings = nn.ModuleDict({
            name: nn.Embedding(vocab_size, emb_dim)
            for name, (vocab_size, emb_dim) in embedding_dims.items()
        })

        # Deep component (DNN)
        self.deep = nn.Sequential(
            nn.Linear(deep_dim, 512),
            nn.ReLU(),
            nn.BatchNorm1d(512),
            nn.Dropout(0.3),

            nn.Linear(512, 256),
            nn.ReLU(),
            nn.BatchNorm1d(256),
            nn.Dropout(0.2),

            nn.Linear(256, 128),
            nn.ReLU(),
            nn.BatchNorm1d(128),

            nn.Linear(128, 1)
        )

    def forward(self, wide_features, categorical_features, numeric_features):
        # Wide part
        wide_out = self.wide(wide_features)

        # Embeddings
        emb_outputs = []
        for name, ids in categorical_features.items():
            emb_outputs.append(self.embeddings[name](ids))

        # Concatenate embeddings + numeric features
        deep_input = torch.cat(emb_outputs + [numeric_features], dim=-1)

        # Deep part
        deep_out = self.deep(deep_input)

        # Combine
        output = wide_out + deep_out
        return torch.sigmoid(output)

# è¨“ç·´
model = WideAndDeepModel(
    wide_dim=100,
    deep_dim=300,
    embedding_dims={
        'user_id': (1000000, 64),
        'item_id': (10000000, 64),
        'category': (1000, 32),
        'brand': (5000, 32)
    }
)

optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
criterion = nn.BCELoss()

for epoch in range(10):
    for batch in train_loader:
        wide_feat, cat_feat, num_feat, labels = batch

        predictions = model(wide_feat, cat_feat, num_feat)
        loss = criterion(predictions, labels)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
```

### 3. å¯¦æ™‚ç‰¹å¾µæœå‹™

```python
# feature/realtime_feature_service.py
import redis
import json
from datetime import datetime, timedelta

class RealtimeFeatureService:
    def __init__(self, redis_client):
        self.redis = redis_client

    def update_user_action(self, user_id, item_id, action_type):
        """æ›´æ–°ç”¨æˆ¶å¯¦æ™‚è¡Œç‚º"""
        # 1. æœ€è¿‘é»æ“Šåºåˆ—
        key = f"user:{user_id}:recent_clicks"
        self.redis.lpush(key, json.dumps({
            'item_id': item_id,
            'action': action_type,
            'timestamp': datetime.now().isoformat()
        }))
        self.redis.ltrim(key, 0, 99)  # åªä¿ç•™æœ€è¿‘ 100 å€‹
        self.redis.expire(key, 86400)  # 24 å°æ™‚éæœŸ

        # 2. Session å…§è¡Œç‚º
        session_key = f"user:{user_id}:session:{self._get_session_id()}"
        self.redis.sadd(session_key, item_id)
        self.redis.expire(session_key, 1800)  # 30 åˆ†é˜éæœŸ

        # 3. é¡åˆ¥åå¥½ï¼ˆå¯¦æ™‚æ›´æ–°ï¼‰
        category_id = self._get_item_category(item_id)
        category_key = f"user:{user_id}:category_pref"
        self.redis.zincrby(category_key, 1, category_id)
        self.redis.expire(category_key, 604800)  # 7 å¤©éæœŸ

    def get_user_realtime_features(self, user_id):
        """ç²å–ç”¨æˆ¶å¯¦æ™‚ç‰¹å¾µ"""
        features = {}

        # 1. æœ€è¿‘é»æ“Šçš„å•†å“
        recent_key = f"user:{user_id}:recent_clicks"
        recent_clicks = self.redis.lrange(recent_key, 0, 9)
        features['recent_items'] = [
            json.loads(c)['item_id'] for c in recent_clicks
        ]

        # 2. Session å…§ç€è¦½çš„å•†å“æ•¸
        session_key = f"user:{user_id}:session:{self._get_session_id()}"
        features['session_item_count'] = self.redis.scard(session_key)

        # 3. ç†±é–€é¡åˆ¥åå¥½
        category_key = f"user:{user_id}:category_pref"
        top_categories = self.redis.zrevrange(category_key, 0, 4, withscores=True)
        features['top_categories'] = [
            {'category_id': int(c), 'score': s}
            for c, s in top_categories
        ]

        return features

    def update_item_popularity(self, item_id):
        """æ›´æ–°å•†å“å¯¦æ™‚ç†±åº¦"""
        # ä½¿ç”¨ HyperLogLog çµ±è¨ˆç¨ç«‹è¨ªå®¢æ•¸
        hour_key = f"item:{item_id}:uv:{datetime.now().hour}"
        self.redis.pfadd(hour_key, user_id)
        self.redis.expire(hour_key, 7200)

        # é»æ“Šè¨ˆæ•¸ï¼ˆæ»‘å‹•çª—å£ï¼‰
        click_key = f"item:{item_id}:clicks"
        self.redis.incr(click_key)
        self.redis.expire(click_key, 3600)  # 1 å°æ™‚çª—å£

    def get_item_popularity_features(self, item_id):
        """ç²å–å•†å“ç†±åº¦ç‰¹å¾µ"""
        # æœ€è¿‘ 1 å°æ™‚çš„ç¨ç«‹è¨ªå®¢æ•¸
        current_hour = datetime.now().hour
        uv_key = f"item:{item_id}:uv:{current_hour}"
        uv_count = self.redis.pfcount(uv_key)

        # æœ€è¿‘ 1 å°æ™‚çš„é»æ“Šæ•¸
        click_key = f"item:{item_id}:clicks"
        click_count = self.redis.get(click_key) or 0

        return {
            'hourly_uv': uv_count,
            'hourly_clicks': int(click_count),
            'ctr': click_count / max(uv_count, 1)
        }
```

### 4. é‡æ’åºæœå‹™

```python
# rerank/reranker.py
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

class Reranker:
    def __init__(self, config):
        self.config = config

    def rerank(self, user, items, scores):
        """
        æ‡‰ç”¨å¤šç¨®é‡æ’åºç­–ç•¥
        """
        # 1. MMR å¤šæ¨£æ€§
        if self.config.get('diversity_enabled'):
            items, scores = self.apply_mmr(items, scores, lambda_param=0.7)

        # 2. æ–°é®®åº¦æå‡
        if self.config.get('freshness_enabled'):
            scores = self.apply_freshness_boost(items, scores)

        # 3. æ¥­å‹™è¦å‰‡
        items, scores = self.apply_business_rules(user, items, scores)

        # 4. å¤šç›®æ¨™å„ªåŒ–
        if self.config.get('multi_objective'):
            scores = self.apply_multi_objective(user, items, scores)

        # æœ€çµ‚æ’åº
        sorted_indices = np.argsort(scores)[::-1]
        return [items[i] for i in sorted_indices], [scores[i] for i in sorted_indices]

    def apply_mmr(self, items, scores, lambda_param=0.7, top_k=10):
        """
        MMR (Maximal Marginal Relevance) å¤šæ¨£æ€§å„ªåŒ–
        """
        selected_indices = []
        remaining_indices = list(range(len(items)))

        # è¨ˆç®—å•†å“ä¹‹é–“çš„ç›¸ä¼¼åº¦çŸ©é™£
        item_embeddings = np.array([item.embedding for item in items])
        similarity_matrix = cosine_similarity(item_embeddings)

        # é¸æ“‡ç¬¬ä¸€å€‹ï¼ˆåˆ†æ•¸æœ€é«˜ï¼‰
        first_idx = np.argmax(scores)
        selected_indices.append(first_idx)
        remaining_indices.remove(first_idx)

        # è¿­ä»£é¸æ“‡
        while len(selected_indices) < top_k and remaining_indices:
            mmr_scores = []

            for idx in remaining_indices:
                # ç›¸é—œæ€§åˆ†æ•¸
                relevance = scores[idx]

                # èˆ‡å·²é¸å•†å“çš„æœ€å¤§ç›¸ä¼¼åº¦
                max_sim = max([
                    similarity_matrix[idx][s_idx]
                    for s_idx in selected_indices
                ])

                # MMR åˆ†æ•¸
                mmr = lambda_param * relevance - (1 - lambda_param) * max_sim
                mmr_scores.append(mmr)

            # é¸æ“‡ MMR åˆ†æ•¸æœ€é«˜çš„
            best_idx = remaining_indices[np.argmax(mmr_scores)]
            selected_indices.append(best_idx)
            remaining_indices.remove(best_idx)

        return [items[i] for i in selected_indices], [scores[i] for i in selected_indices]

    def apply_freshness_boost(self, items, scores):
        """æ–°é®®åº¦åŠ æ¬Š"""
        boosted_scores = []
        current_time = datetime.now()

        for item, score in zip(items, scores):
            age_days = (current_time - item.created_at).days

            if age_days <= 3:
                boost = 1.5
            elif age_days <= 7:
                boost = 1.3
            elif age_days <= 14:
                boost = 1.1
            else:
                boost = 1.0

            boosted_scores.append(score * boost)

        return np.array(boosted_scores)

    def apply_business_rules(self, user, items, scores):
        """æ¥­å‹™è¦å‰‡éæ¿¾èˆ‡èª¿æ•´"""
        filtered_items = []
        filtered_scores = []

        for item, score in zip(items, scores):
            # ç¡¬ç´„æŸï¼šå¿…é ˆæ»¿è¶³
            if item.stock <= 0:
                continue  # ç„¡åº«å­˜ï¼Œè·³é

            if item.id in user.purchased_items:
                continue  # å·²è³¼è²·ï¼Œè·³é

            # è»Ÿç´„æŸï¼šèª¿æ•´åˆ†æ•¸
            if item.profit_margin > 0.5:
                score *= 1.2  # é«˜åˆ©æ½¤å•†å“åŠ æ¬Š

            if item.sales_count > 10000:
                score *= 1.1  # ç†±éŠ·å•†å“åŠ æ¬Š

            if user.is_vip:
                score *= 1.15  # VIP ç”¨æˆ¶çœ‹åˆ°æ›´å¥½çš„å•†å“

            filtered_items.append(item)
            filtered_scores.append(score)

        return filtered_items, np.array(filtered_scores)

    def apply_multi_objective(self, user, items, scores):
        """å¤šç›®æ¨™å„ªåŒ–"""
        # é æ¸¬å¤šå€‹ç›®æ¨™
        ctr_scores = self.ctr_model.predict(user, items)
        cvr_scores = self.cvr_model.predict(user, items)

        # è¨ˆç®—é æœŸåˆ©æ½¤
        profits = np.array([item.price * item.profit_margin for item in items])

        # æ­£è¦åŒ–
        ctr_norm = (ctr_scores - ctr_scores.min()) / (ctr_scores.max() - ctr_scores.min())
        cvr_norm = (cvr_scores - cvr_scores.min()) / (cvr_scores.max() - cvr_scores.min())
        profit_norm = (profits - profits.min()) / (profits.max() - profits.min())

        # åŠ æ¬Šçµ„åˆ
        weights = self.config['multi_objective_weights']
        final_scores = (
            weights['ctr'] * ctr_norm +
            weights['cvr'] * cvr_norm +
            weights['profit'] * profit_norm
        )

        return final_scores
```

## API æ–‡ä»¶

### 1. ç²å–å€‹æ€§åŒ–æ¨è–¦

```http
POST /api/v1/recommend
Content-Type: application/json
Authorization: Bearer <token>

{
    "user_id": 123456,
    "scene": "homepage",      // å ´æ™¯ï¼šhomepage, detail_page, cart
    "num_items": 10,
    "context": {
        "device": "mobile",
        "location": "taipei",
        "time": "2025-01-15T10:00:00Z"
    }
}

Response 200 OK:
{
    "request_id": "req_550e8400",
    "items": [
        {
            "item_id": 789012,
            "title": "iPhone 15 Pro Max",
            "price": 39900,
            "image_url": "https://cdn.example.com/iphone15.jpg",
            "score": 0.92,
            "reason": "åŸºæ–¼ä½ çš„ç€è¦½æ­·å²",
            "source": "collaborative_filtering"
        },
        {
            "item_id": 345678,
            "title": "AirPods Pro",
            "price": 7490,
            "image_url": "https://cdn.example.com/airpods.jpg",
            "score": 0.88,
            "reason": "ç¶“å¸¸ä¸€èµ·è³¼è²·",
            "source": "item_similarity"
        }
    ],
    "latency_ms": 85
}
```

### 2. ç›¸ä¼¼å•†å“æ¨è–¦

```http
GET /api/v1/items/{item_id}/similar?limit=20
Authorization: Bearer <token>

Response 200 OK:
{
    "item_id": 789012,
    "similar_items": [
        {
            "item_id": 789013,
            "title": "iPhone 15 Pro",
            "similarity_score": 0.95,
            "similarity_type": "collaborative"
        },
        {
            "item_id": 789011,
            "title": "iPhone 14 Pro Max",
            "similarity_score": 0.87,
            "similarity_type": "content"
        }
    ]
}
```

### 3. è¨˜éŒ„ç”¨æˆ¶è¡Œç‚º

```http
POST /api/v1/events
Content-Type: application/json
Authorization: Bearer <token>

{
    "user_id": 123456,
    "events": [
        {
            "event_type": "view",
            "item_id": 789012,
            "timestamp": "2025-01-15T10:00:00Z",
            "duration_seconds": 30,
            "context": {
                "page": "detail",
                "source": "recommendation"
            }
        },
        {
            "event_type": "click",
            "item_id": 789012,
            "timestamp": "2025-01-15T10:00:30Z"
        }
    ]
}

Response 200 OK:
{
    "message": "Events recorded successfully",
    "event_count": 2
}
```

### 4. A/B Testing åˆ†é…

```http
GET /api/v1/ab/assign?user_id=123456&experiment=rec_algo_v2
Authorization: Bearer <token>

Response 200 OK:
{
    "experiment_id": "exp_123",
    "experiment_name": "rec_algo_v2",
    "variant": "treatment",
    "config": {
        "recall_strategies": ["cf", "content", "deep_learning"],
        "ranking_model": "wide_and_deep_v2"
    }
}
```

## æ•ˆèƒ½å„ªåŒ–

### 1. å‘é‡æª¢ç´¢å„ªåŒ–ï¼ˆFaissï¼‰

```python
import faiss
import numpy as np

class FaissIndex:
    def __init__(self, dimension=128):
        self.dimension = dimension
        # ä½¿ç”¨ IVF (Inverted File) + PQ (Product Quantization)
        self.index = faiss.IndexIVFPQ(
            faiss.IndexFlatL2(dimension),
            dimension,
            nlist=1000,        # èšé¡ä¸­å¿ƒæ•¸é‡
            m=64,              # PQ å­å‘é‡æ•¸é‡
            nbits=8            # æ¯å€‹å­å‘é‡çš„ä½å…ƒæ•¸
        )

    def train(self, vectors):
        """è¨“ç·´ç´¢å¼•"""
        self.index.train(vectors)
        self.index.add(vectors)

    def search(self, query_vector, top_k=100):
        """æª¢ç´¢æœ€ç›¸ä¼¼çš„å‘é‡"""
        self.index.nprobe = 10  # æœå°‹çš„èšé¡æ•¸
        distances, indices = self.index.search(query_vector, top_k)
        return indices[0], distances[0]

# æ•ˆèƒ½æ¯”è¼ƒ
# Flat Index: 100 è¬å‘é‡ï¼Œæœå°‹æ™‚é–“ ~500ms
# IVF+PQ: 100 è¬å‘é‡ï¼Œæœå°‹æ™‚é–“ ~20msï¼ˆ25Ã— åŠ é€Ÿï¼‰
# æº–ç¢ºåº¦å½±éŸ¿ï¼š< 2%
```

### 2. æ¨¡å‹æœå‹™å„ªåŒ–

```python
# ä½¿ç”¨ TensorFlow Serving æ‰¹æ¬¡æ¨ç†
import tensorflow as tf

# æ¨¡å‹æ‰¹æ¬¡é…ç½®
batching_parameters = """
max_batch_size { value: 128 }
batch_timeout_micros { value: 5000 }
max_enqueued_batches { value: 100 }
num_batch_threads { value: 8 }
"""

# éƒ¨ç½²æ™‚å•Ÿç”¨æ‰¹æ¬¡
tensorflow_model_server \
    --rest_api_port=8501 \
    --model_name=ranking_model \
    --model_base_path=/models/ranking \
    --enable_batching=true \
    --batching_parameters_file=batching_config.txt

# æ•ˆèƒ½æå‡ï¼š
# å–®æ¬¡æ¨ç†ï¼š10ms Ã— 100 è«‹æ±‚ = 1000ms
# æ‰¹æ¬¡æ¨ç†ï¼ˆbatch=100ï¼‰ï¼š50msï¼ˆ20Ã— åŠ é€Ÿï¼‰
```

### 3. ç‰¹å¾µå¿«å–

```python
class FeatureCache:
    def __init__(self, redis_client, ttl=3600):
        self.redis = redis_client
        self.ttl = ttl

    def get_user_features(self, user_id):
        """ç²å–ç”¨æˆ¶ç‰¹å¾µï¼ˆå¸¶å¿«å–ï¼‰"""
        cache_key = f"user_features:{user_id}"

        # å˜—è©¦å¾å¿«å–ç²å–
        cached = self.redis.get(cache_key)
        if cached:
            return json.loads(cached)

        # å¾è³‡æ–™åº«è¼‰å…¥
        features = self.load_from_db(user_id)

        # å¯«å…¥å¿«å–
        self.redis.setex(cache_key, self.ttl, json.dumps(features))

        return features

# æ•ˆèƒ½æå‡ï¼š
# è³‡æ–™åº«æŸ¥è©¢ï¼š20ms
# Redis å¿«å–ï¼š< 1msï¼ˆ20Ã— åŠ é€Ÿï¼‰
# å¿«å–å‘½ä¸­ç‡ï¼š85%
```

## éƒ¨ç½²æ¶æ§‹

```yaml
# kubernetes/recommendation-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: recommendation-service
spec:
  replicas: 20
  selector:
    matchLabels:
      app: recommendation
  template:
    spec:
      containers:
      - name: recommendation
        image: recommendation:v1.0.0
        resources:
          requests:
            memory: "4Gi"
            cpu: "2000m"
          limits:
            memory: "8Gi"
            cpu: "4000m"
        env:
        - name: REDIS_URL
          value: "redis://redis-cluster:6379"
        - name: POSTGRES_URL
          valueFrom:
            secretKeyRef:
              name: db-secret
              key: url
        - name: MODEL_SERVING_URL
          value: "http://tf-serving:8501"

---
# TensorFlow Serving for ranking model
apiVersion: apps/v1
kind: Deployment
metadata:
  name: tf-serving
spec:
  replicas: 10
  template:
    spec:
      containers:
      - name: tensorflow-serving
        image: tensorflow/serving:latest
        args:
          - --model_name=ranking_model
          - --model_base_path=/models/ranking
          - --rest_api_port=8501
          - --enable_batching=true
        resources:
          limits:
            nvidia.com/gpu: 1
        volumeMounts:
        - name: model-storage
          mountPath: /models

---
# Faiss vector search service
apiVersion: apps/v1
kind: Deployment
metadata:
  name: vector-search
spec:
  replicas: 5
  template:
    spec:
      containers:
      - name: faiss-server
        image: faiss-server:v1.0.0
        resources:
          requests:
            memory: "16Gi"
            cpu: "4000m"
```

## æˆæœ¬ä¼°ç®—

### æ¯æœˆé‹ç‡Ÿæˆæœ¬ï¼ˆ1000 è¬ DAUï¼Œæ¯äººæ¯å¤© 20 æ¬¡æ¨è–¦ï¼‰

| é …ç›® | ç”¨é‡ | å–®åƒ¹ | æœˆæˆæœ¬ |
|------|------|------|--------|
| **é‹ç®—è³‡æº** | | | |
| API æœå‹™ | 20 Ã— c5.2xlarge | $0.34/hr | $4,896 |
| TF Serving (GPU) | 10 Ã— p3.2xlarge | $3.06/hr | $22,032 |
| Vector Search | 5 Ã— r5.4xlarge | $1.008/hr | $3,629 |
| **å„²å­˜** | | | |
| Redis Cluster | 3 Ã— r5.4xlarge | $1.008/hr | $2,177 |
| PostgreSQL | db.r5.4xlarge | $1.008/hr | $726 |
| HBase (EC2) | 10 Ã— i3.2xlarge | $0.624/hr | $4,493 |
| S3 (æ¨¡å‹/æ—¥èªŒ) | 50TB | $0.023/GB | $1,150 |
| **è³‡æ–™è™•ç†** | | | |
| Kafka | 5 Ã— r5.xlarge | $0.252/hr | $907 |
| Spark (é›¢ç·šè¨“ç·´) | 20 Ã— r5.2xlarge | $0.504/hr | $7,258 |
| **ç¶²è·¯** | | | |
| Data Transfer | 100TB | $0.09/GB | $9,000 |
| **ç›£æ§** | | | |
| Prometheus + Grafana | - | - | $500 |
| **ç¸½è¨ˆ** | | | **$56,768** |

### æˆæœ¬å„ªåŒ–ç­–ç•¥

**å„ªåŒ–å¾Œæˆæœ¬ï¼š$34,061ï¼ˆé™ä½ 40%ï¼‰**

1. **Spot Instances for è¨“ç·´**ï¼šSpark æˆæœ¬é™ä½ 70% = ç¯€çœ $5,081
2. **æ¨¡å‹é‡åŒ– (INT8)**ï¼šGPU éœ€æ±‚æ¸›åŠ = ç¯€çœ $11,016
3. **Faiss é‡åŒ–**ï¼šè¨˜æ†¶é«”éœ€æ±‚é™ä½ 50% = ç¯€çœ $1,815
4. **Redis æ™ºæ…§éæœŸ**ï¼šè¨˜æ†¶é«”ç¯€çœ 30% = ç¯€çœ $653
5. **Reserved Instancesï¼ˆ1 å¹´ï¼‰**ï¼šåŸºç¤è¨­æ–½é™ä½ 30% = ç¯€çœ $3,142

### ROI åˆ†æ

**æ¥­å‹™åƒ¹å€¼ï¼š**
- GMV æå‡ï¼š$100M â†’ $250Mï¼ˆ+$150M/æœˆï¼‰
- æ¨è–¦è²¢ç»æ¯”ä¾‹ï¼š35%
- æ­¸å› æ–¼æ¨è–¦ç³»çµ±çš„å¢é‡ GMVï¼š$52.5M/æœˆ
- åˆ©æ½¤ç‡ï¼š10%
- **å¢é‡åˆ©æ½¤ï¼š$5.25M/æœˆ**

**ROI = (å¢é‡åˆ©æ½¤ - ç³»çµ±æˆæœ¬) / ç³»çµ±æˆæœ¬**
**ROI = ($5,250,000 - $34,061) / $34,061 = 15,303%**

## ç›£æ§èˆ‡å‘Šè­¦

```yaml
# Prometheus å‘Šè­¦è¦å‰‡
groups:
  - name: recommendation_system
    rules:
      # æ¨è–¦å»¶é²éé«˜
      - alert: HighRecommendationLatency
        expr: histogram_quantile(0.99, rate(recommendation_latency_seconds_bucket[5m])) > 0.2
        for: 5m
        annotations:
          summary: "P99 æ¨è–¦å»¶é² > 200ms"

      # å¬å›æ•¸é‡ä¸è¶³
      - alert: LowRecallCount
        expr: avg(recall_candidate_count) < 100
        for: 10m
        annotations:
          summary: "å¬å›å€™é¸æ•¸é‡ < 100"

      # CTR ä¸‹é™
      - alert: CTRDrop
        expr: rate(recommendation_clicks_total[1h]) / rate(recommendation_impressions_total[1h]) < 0.05
        for: 30m
        annotations:
          summary: "CTR < 5%ï¼Œå¯èƒ½æ¨¡å‹é™ç´š"

      # æ¨¡å‹æœå‹™ä¸å¯ç”¨
      - alert: ModelServingDown
        expr: up{job="tf-serving"} == 0
        for: 2m
        annotations:
          summary: "TensorFlow Serving ç„¡æ³•é€£ç·š"
```

## ç¸½çµ

æ¨è–¦å¼•æ“é€éå¤šç­–ç•¥å¬å›ã€æ·±åº¦å­¸ç¿’æ’åºã€æ™ºèƒ½é‡æ’åºï¼Œæ‰“é€ å€‹æ€§åŒ–é«”é©—ï¼š

| æ¨¡çµ„ | æŠ€è¡“ | åƒ¹å€¼ |
|------|------|------|
| **å¬å›** | å”åŒéæ¿¾ + å…§å®¹ + æ·±åº¦å­¸ç¿’ | è¦†è“‹ç‡ > 95% |
| **æ’åº** | Wide & Deep / DCN | CTR æå‡ 4Ã— |
| **é‡æ’åº** | MMR + æ¥­å‹™è¦å‰‡ | å¤šæ¨£æ€§ +30% |
| **å¯¦æ™‚** | Redis + Kafka | å»¶é² < 100ms |
| **A/B Testing** | å¯¦é©—æ¡†æ¶ | æŒçºŒå„ªåŒ– |

é€éæœ¬ç« å­¸ç¿’ï¼Œä½ æŒæ¡äº†ï¼š

1. âœ… **å”åŒéæ¿¾**ï¼šItem-CFã€ALS å¤§è¦æ¨¡è¨“ç·´
2. âœ… **æ·±åº¦å­¸ç¿’**ï¼šTwo-Towerã€Wide & Deepã€DCN
3. âœ… **å¯¦æ™‚ç‰¹å¾µ**ï¼šRedis Feature Storeã€åºåˆ—å»ºæ¨¡
4. âœ… **é‡æ’åº**ï¼šMMR å¤šæ¨£æ€§ã€æ–°é®®åº¦ã€æ¥­å‹™è¦å‰‡
5. âœ… **å‘é‡æª¢ç´¢**ï¼šFaiss ANNã€IVF+PQ å„ªåŒ–
6. âœ… **A/B Testing**ï¼šå¯¦é©—è¨­è¨ˆã€æ•ˆæœè©•ä¼°
7. âœ… **å®Œæ•´æ¶æ§‹**ï¼šå¾å¬å›åˆ°éƒ¨ç½²çš„ç”Ÿç”¢ç´šç³»çµ±

**Phase 7: AI Platforms å®Œæˆï¼** ğŸ‰
