package main

import (
	"flag"
	"log"
	"time"

	"11-web-crawler/internal"
)

func main() {
	// è§£æå‘½ä»¤è¡Œåƒæ•¸
	workers := flag.Int("workers", 5, "Number of concurrent workers")
	maxDepth := flag.Int("max-depth", 3, "Maximum crawl depth")
	userAgent := flag.String("user-agent", "SimpleC rawler/1.0", "User-Agent string")
	seedURL := flag.String("seed", "https://example.com", "Seed URL to start crawling")
	flag.Parse()

	log.Println("ğŸš€ Web Crawler Starting...")
	log.Printf("âš™ï¸  Workers: %d", *workers)
	log.Printf("ğŸ“Š Max Depth: %d", *maxDepth)
	log.Printf("ğŸŒ Seed URL: %s", *seedURL)

	// å‰µå»ºçˆ¬èŸ²é…ç½®
	config := &internal.Config{
		WorkerCount:   *workers,
		MaxDepth:      *maxDepth,
		UserAgent:     *userAgent,
		RespectRobots: true,
		CrawlDelay:    1 * time.Second,
		MaxURLs:       10000,
	}

	// å‰µå»ºçˆ¬èŸ²
	crawler := internal.NewCrawler(config)

	// è¨­ç½®è™•ç†å™¨ï¼ˆæ‰“å°çˆ¬å–çµæœï¼‰
	crawler.SetHandler(func(url string, content []byte) {
		log.Printf("âœ… Crawled: %s (size: %d bytes)", url, len(content))
		// é€™è£¡å¯ä»¥æ·»åŠ è‡ªå®šç¾©é‚è¼¯ï¼š
		// - è§£æåƒ¹æ ¼
		// - æå–éˆæ¥
		// - å­˜å…¥æ•¸æ“šåº«
	})

	// æ·»åŠ ç¨®å­ URL
	crawler.AddSeed(*seedURL, 0) // å„ªå…ˆç´š 0ï¼ˆæœ€é«˜ï¼‰

	// å•Ÿå‹•çˆ¬å–
	crawler.Start()

	log.Println("ğŸ‰ Crawler finished!")
	log.Printf("ğŸ“ˆ Stats: %+v", crawler.GetStats())
}
